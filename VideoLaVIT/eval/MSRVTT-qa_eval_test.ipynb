{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6a3e3c-7533-4c45-b9c7-583b9a3f523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80dc201-8ad3-4daf-9253-9b773ffb9293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 2.5.1+cu124)\n",
      "    Python  3.10.11 (you have 3.10.15)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/triton/softmax.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16 if _triton_softmax_fp16_enabled else None)\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/triton/softmax.py:87: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:107: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23596298-fc7c-478a-a90b-23e67c8f6d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12278\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('val_qa_msrvtt.json') as f:\n",
    "    dataset = json.load(f)\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309aac64-2653-44dc-b47a-64760f1515a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'coversation',\n",
       "  'category_id': 14,\n",
       "  'id': 158582,\n",
       "  'question': 'what is a family having?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'music',\n",
       "  'category_id': 14,\n",
       "  'id': 158583,\n",
       "  'question': 'what plays?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'music',\n",
       "  'category_id': 14,\n",
       "  'id': 158584,\n",
       "  'question': 'what is playing?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'couch',\n",
       "  'category_id': 14,\n",
       "  'id': 158585,\n",
       "  'question': 'what does the girl sing on?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'feature',\n",
       "  'category_id': 14,\n",
       "  'id': 158586,\n",
       "  'question': 'what is a music video doing?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'sit',\n",
       "  'category_id': 14,\n",
       "  'id': 158587,\n",
       "  'question': 'what are people doing?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'sit',\n",
       "  'category_id': 14,\n",
       "  'id': 158588,\n",
       "  'question': 'what are three people doing?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'sit',\n",
       "  'category_id': 14,\n",
       "  'id': 158589,\n",
       "  'question': 'what are three young people doing?',\n",
       "  'video_id': 6513},\n",
       " {'answer': 'talk',\n",
       "  'category_id': 14,\n",
       "  'id': 158590,\n",
       "  'question': 'what are two girls and a guy doing?',\n",
       "  'video_id': 6513}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e022f7f-342e-4168-8ec7-6f0aa3a337f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/YouTubeClips/W8yjP35v1EU.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47091f6-6bc9-48a2-9615-16706cd7d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "id2video_mapper = defaultdict()\n",
    "\n",
    "with open(\"youtube_mapping.txt\") as file:\n",
    "    lines = [line.rstrip() for line in file]\n",
    "\n",
    "for line in lines:\n",
    "    key, value = line.split()\n",
    "    value = value.split('vid')[1]\n",
    "    id2video_mapper[value] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081263bf-b56f-46d2-b5cc-371eed39f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class msrvttDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, module_path, dataset, mapper, task_type = 'qa', question_prompt = \" Answer the question using a single word or a short phrase with multiple words.\"):\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.question_prompt = question_prompt\n",
    "\n",
    "        self.movie_dir = module_path\n",
    "\n",
    "        for elem in dataset:\n",
    "            full_video_path = module_path + '/eval/YouTubeClips' +'/' + mapper[str(elem['video_id'])] +\".avi\"\n",
    "            self.data_list.append({'full_video_path':full_video_path, 'question':elem['question'], 'answer':elem['answer']})\n",
    "            \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data_list[idx]\n",
    "        video_path = data_item['full_video_path']\n",
    "        \n",
    "        if not os.path.exists(video_path):\n",
    "            print (video_path)\n",
    "            print(f\"Warning: Video file not found at {video_path}, skipping this item.\")\n",
    "            return None  \n",
    "    \n",
    "\n",
    "        question = data_item['question']\n",
    "        answer = data_item['answer']\n",
    "    \n",
    "        return {\n",
    "            'question': question.capitalize() + self.question_prompt,\n",
    "            'video_path': video_path,\n",
    "            #'pixel_values': pixel_values,\n",
    "            'answer': answer.capitalize(),\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04eb3d5-ca74-4372-9341-276816df3ac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'6513'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m msrvtt \u001b[38;5;241m=\u001b[39m \u001b[43mmsrvttDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2video_mapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mmsrvttDataset.__init__\u001b[0;34m(self, module_path, dataset, mapper, task_type, question_prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_dir \u001b[38;5;241m=\u001b[39m module_path\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 11\u001b[0m     full_video_path \u001b[38;5;241m=\u001b[39m module_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/eval/YouTubeClips\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mmapper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_video_path\u001b[39m\u001b[38;5;124m'\u001b[39m:full_video_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m:elem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m:elem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "\u001b[0;31mKeyError\u001b[0m: '6513'"
     ]
    }
   ],
   "source": [
    "msrvtt = msrvttDataset(module_path, dataset, id2video_mapper, task_type = 'qa') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197bfdb-efc6-4600-9ae8-047034cf1538",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c657c4-25f7-4a08-9ee7-29670cfe4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video LaVIT Model Weight from /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft, model precision: bf16\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5653513122846d191b505575349645c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.embedding.initted', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.cluster_size']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Visual Vocab Size is 16384\n",
      "The llama tokenizer vocab size is 32000\n",
      "The maximal clip number is 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "max_video_clips = 16\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# For Multi-Modal Understanding\n",
    "runner = build_model(model_path=model_path, model_dtype=model_dtype, understanding=True, \n",
    "        device_id=device_id, use_xformers=False, max_video_clips=max_video_clips,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eaef305-f3d8-4560-aa4b-c7783d08ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Man\n"
     ]
    }
   ],
   "source": [
    "video_path = '/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/YouTubeClips/bQJQGoJF7_k_162_169.avi'\n",
    "prompt = \"Who pours marinade in a bag of chicken? Answer the question using a single word or phrase.\"\n",
    "\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7b3e67-c5a4-4f09-b07f-69c66f34a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab567f4-de77-4e0e-92d1-3165987be755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who pours liquid from a plastic container into a ziploc bag containing meat pieces? Answer the question using a single word or a short phrase with multiple words.']\n",
      "['Someone']\n",
      "['/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/YouTubeClips/bQJQGoJF7_k_162_169.avi']\n",
      "['Who pours a seasoning liquid from a plastic container over chicken pieces placed in a plastic pouch? Answer the question using a single word or a short phrase with multiple words.']\n",
      "['Man']\n",
      "['/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/YouTubeClips/bQJQGoJF7_k_162_169.avi']\n",
      "['Who pours marinade in a bag of chicken? Answer the question using a single word or a short phrase with multiple words.']\n",
      "['Person']\n",
      "['/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/YouTubeClips/bQJQGoJF7_k_162_169.avi']\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batches, tokenizer):\n",
    "    \n",
    "    questions = [_['question'] for _ in batches]\n",
    "    video_path = [_['video_path'] for _ in batches]\n",
    "    answer = [_['answer'] for _ in batches]\n",
    "    \n",
    "    return questions, video_path, answer\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=msvdq,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=partial(collate_fn, tokenizer=tokenizer)\n",
    "    )\n",
    "\n",
    "iterator = iter(dataloader)\n",
    "first_batch = next(iterator)\n",
    "\n",
    "\n",
    "question = first_batch[0]\n",
    "video_path = first_batch[1]\n",
    "answer = first_batch[2]\n",
    "\n",
    "\n",
    "print(question)\n",
    "print(answer)\n",
    "print(video_path)\n",
    "\n",
    "\n",
    "first_batch = next(iterator)\n",
    "\n",
    "\n",
    "question = first_batch[0]\n",
    "video_path = first_batch[1]\n",
    "answer = first_batch[2]\n",
    "\n",
    "\n",
    "print(question)\n",
    "print(answer)\n",
    "print(video_path)\n",
    "\n",
    "\n",
    "first_batch = next(iterator)\n",
    "\n",
    "\n",
    "question = first_batch[0]\n",
    "video_path = first_batch[1]\n",
    "answer = first_batch[2]\n",
    "\n",
    "\n",
    "print(question)\n",
    "print(answer)\n",
    "print(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d54183-de3a-4985-ab27-e8be2dbf5f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|███████   | 4499/6415 [1:46:04<27:06,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Who is pouring olive oil into a pan? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Chef\n",
      "====================real answers====================\n",
      "Someone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▍  | 4799/6415 [1:50:46<24:42,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What is a woman seated by a lake pulled at by a black gloved hand? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Grabbed\n",
      "====================real answers====================\n",
      "Ankle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▋  | 4899/6415 [1:52:09<24:08,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What is karate kicking a person? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Karate\n",
      "====================real answers====================\n",
      "Monkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 5299/6415 [2:00:53<18:31,  1.00it/s]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What are two players playing? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Ping pong\n",
      "====================real answers====================\n",
      "Tenni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  84%|████████▍ | 5399/6415 [2:03:26<18:14,  1.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What are two men doing? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "One man is beating and kicking the other man.\n",
      "====================real answers====================\n",
      "Struggle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 5499/6415 [2:05:42<33:34,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Who plays with a plastic tea set? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Baby\n",
      "====================real answers====================\n",
      "Toddler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  87%|████████▋ | 5599/6415 [2:07:47<13:33,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What is someone doing? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Someone picks up a little baby tiger from a couch and waves to the camera at the end.\n",
      "====================real answers====================\n",
      "Pet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 5699/6415 [2:11:31<34:52,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What is a man spooning onto a tray? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Pancake\n",
      "====================real answers====================\n",
      "Pancake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|█████████ | 5799/6415 [2:14:18<07:43,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Who is dancing outside? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Man\n",
      "====================real answers====================\n",
      "Woman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  92%|█████████▏| 5899/6415 [2:15:54<06:23,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What is someone doing? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Cutting\n",
      "====================real answers====================\n",
      "Cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▎| 5999/6415 [2:17:18<04:13,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What does a baby panda fall in? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Snow\n",
      "====================real answers====================\n",
      "Snow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▌| 6099/6415 [2:19:25<13:41,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What is someone kneeding? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "Dough\n",
      "====================real answers====================\n",
      "Dough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 6199/6415 [2:21:11<02:40,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "What does a man cut a chicken with? Answer the question using a single word or a short phrase with multiple words.\n",
      "====================output====================\n",
      "The man cuts a chicken on the ground\n",
      "====================real answers====================\n",
      "Axe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6415/6415 [2:25:12<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_real = []\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        dataloader, total=len(dataloader), desc=f\"Epoch 1\"\n",
    "    )\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for step, batch in enumerate(progress_bar, start=1):\n",
    "    sample_set = {}\n",
    "    question = batch[0]\n",
    "    video_path = batch[1]\n",
    "    answer = batch[2]\n",
    "\n",
    "\n",
    "    sample_set['q'] = question\n",
    "    sample_set['real'] = answer[0].strip()\n",
    "\n",
    "    # print(pixel_values.size())\n",
    "    # print(question)\n",
    "    # print(answer)\n",
    "    # print(num_patches_list)\n",
    "    # print(task_type)\n",
    "    \n",
    "    # with autocast():\n",
    "        # 执行推理\n",
    "    outputs = runner({\"video\": video_path[0], \"text_input\": question[0]}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "\n",
    "    sample_set['a'] = outputs.strip()\n",
    "\n",
    "    y_pred.append(outputs.strip())\n",
    "    \n",
    "    y_real.append(answer[0].strip())\n",
    "\n",
    "    output_list.append(sample_set)\n",
    "\n",
    "    sample_set['a'] = outputs.strip()\n",
    "    if (step%100 == 0):\n",
    "        #with open('y_pred_msvdqa1.pkl', 'wb') as f:\n",
    "            #pickle.dump(y_pred, f)\n",
    "        #with open('y_real_msvdqa1.pkl', 'wb') as f:\n",
    "            #pickle.dump(y_real, f)\n",
    "\n",
    "        with open(\"sample_set.json\", 'w') as file:\n",
    "            json.dump(output_list, file)\n",
    "\n",
    "        print(\"=\"*20 + \"question\" + \"=\"*20)\n",
    "        print (question[0])\n",
    "        print(\"=\"*20 + \"output\" + \"=\"*20)\n",
    "        print(outputs)\n",
    "        print(\"=\"*20 + \"real answers\" + \"=\"*20)\n",
    "        print(answer[0], flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe21558-6b7f-43ba-a4bb-9739e3544249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Man',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Bbq sauce',\n",
       " 'Man',\n",
       " 'The person pouring marinade into a bag on a kitchen counter is a woman.',\n",
       " 'No one',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Chef',\n",
       " 'Man',\n",
       " 'Sauce',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Marinade',\n",
       " 'Man',\n",
       " 'Sauce',\n",
       " 'Meat',\n",
       " 'Clear plastic bag',\n",
       " 'Plastic container',\n",
       " 'Marinade',\n",
       " 'Chicken',\n",
       " 'Bag',\n",
       " 'Dressing',\n",
       " 'Olive oil',\n",
       " 'Sauce',\n",
       " 'Spices',\n",
       " 'Sauce',\n",
       " 'Chicken',\n",
       " 'Styrofoam container']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854c805-0594-4bec-bc09-3598e7ad5941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Someone',\n",
       " 'Man',\n",
       " 'Person',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Cook',\n",
       " 'Man',\n",
       " 'Person',\n",
       " 'Person',\n",
       " 'Seasoning',\n",
       " 'Man',\n",
       " 'Man',\n",
       " 'Sauce',\n",
       " 'Man',\n",
       " 'Marinade',\n",
       " 'Chicken',\n",
       " 'Bag',\n",
       " 'Bag',\n",
       " 'Bowl',\n",
       " 'Chicken',\n",
       " 'Bag',\n",
       " 'Marinade',\n",
       " 'Sauce',\n",
       " 'Marinade',\n",
       " 'Marinade',\n",
       " 'Sauce',\n",
       " 'Meat',\n",
       " 'Bag']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05463109-a774-41e2-bd6d-d408ed80a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoLAVIT_MSVDQA, accuracy is: 0.21818181818181817\n"
     ]
    }
   ],
   "source": [
    "result = [1 if i == j else 0 for i, j in zip(y_real, y_pred)]\n",
    "\n",
    "print(\"VideoLAVIT_MSVDQA, accuracy is: \" + str(sum(result) / len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f314cd9-f71f-480a-b451-40c0feebbf3a",
   "metadata": {},
   "source": [
    "## Evaluation by Gpt-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b510d34-5eac-47e1-b569-0fc201831775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define the test cases\n",
    "test_cases = [\n",
    "    {\"input\": \"What is the capital of France?\", \"expected_output\": \"Paris\"},\n",
    "    {\"input\": \"What is 2 + 2?\", \"expected_output\": \"4\"},\n",
    "    {\"input\": \"Explain photosynthesis briefly.\", \"expected_output\": \"Photosynthesis is the process by which plants convert sunlight into energy.\"},\n",
    "]\n",
    "\n",
    "# Validate the model\n",
    "for test in test_cases:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": test[\"input\"]}]\n",
    "    )\n",
    "    output = response['choices'][0]['message']['content'].strip()\n",
    "    print(f\"Input: {test['input']}\")\n",
    "    print(f\"Expected: {test['expected_output']}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(f\"Match: {output == test['expected_output']}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

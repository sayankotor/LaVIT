{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494c1162-3d5a-4fd3-b4d1-936195757d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  video_id duration     domain        sub_category  \\\n",
      "0      001    short  Knowledge  Humanity & History   \n",
      "1      001    short  Knowledge  Humanity & History   \n",
      "2      001    short  Knowledge  Humanity & History   \n",
      "3      002    short  Knowledge  Humanity & History   \n",
      "4      002    short  Knowledge  Humanity & History   \n",
      "\n",
      "                                           url      videoID question_id  \\\n",
      "0  https://www.youtube.com/watch?v=fFjv93ACGo8  fFjv93ACGo8       001-1   \n",
      "1  https://www.youtube.com/watch?v=fFjv93ACGo8  fFjv93ACGo8       001-2   \n",
      "2  https://www.youtube.com/watch?v=fFjv93ACGo8  fFjv93ACGo8       001-3   \n",
      "3  https://www.youtube.com/watch?v=N1cdUjctpG8  N1cdUjctpG8       002-1   \n",
      "4  https://www.youtube.com/watch?v=N1cdUjctpG8  N1cdUjctpG8       002-2   \n",
      "\n",
      "              task_type                                           question  \\\n",
      "0      Counting Problem  When demonstrating the Germany modern Christma...   \n",
      "1  Information Synopsis                   What is the genre of this video?   \n",
      "2      Counting Problem  How many red socks are above the fireplace at ...   \n",
      "3    Object Recognition  Which of the following features/items is not d...   \n",
      "4      Action Reasoning  Which of the following reasons motivated the a...   \n",
      "\n",
      "                                             options answer  \n",
      "0  [A. Apples., B. Candles., C. Berries., D. The ...      C  \n",
      "1  [A. It is a news report that introduces the hi...      A  \n",
      "2                       [A. 1., B. 4., C. 2., D. 3.]      D  \n",
      "3  [A. Inkstone., B. Niche., C. Jade., D. Sacrifi...      C  \n",
      "4  [A. Because it's from Ming Dynasty and of spec...      D  \n",
      "When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?\n",
      "['A. Apples.', 'B. Candles.', 'C. Berries.', 'D. The three kinds are of the same number.']\n",
      "C\n",
      "fFjv93ACGo8\n",
      "https://www.youtube.com/watch?v=fFjv93ACGo8\n",
      "Counting Problem\n",
      "When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?\n",
      "https://www.youtube.com/watch?v=fFjv93ACGo8\n",
      "fFjv93ACGo8\n",
      "Counting Problem\n",
      "['A. Apples.', 'B. Candles.', 'C. Berries.', 'D. The three kinds are of the same number.']\n",
      "C\n",
      "1644\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# class ViedoMMEDataset(torch.utils.data.Dataset):\n",
    "\n",
    "file_path = '/home/jovyan/shares/SR004.nfs2/lipengyi/Video-MME/videomme/test-00000-of-00001.parquet'\n",
    "\n",
    "# 使用 pandas 读取 Parquet 文件\n",
    "df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "\n",
    "# 显示 DataFrame 的内容\n",
    "print(df.head())\n",
    "\n",
    "print(df['question'][0])\n",
    "print(df['options'][0])\n",
    "print(df['answer'][0])\n",
    "print(df['videoID'][0])\n",
    "print(df['url'][0])\n",
    "print(df['task_type'][0])\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"lmms-lab/Video-MME\")\n",
    "ds\n",
    "\n",
    "test_dataset = ds['test']\n",
    "test_dataset\n",
    "\n",
    "print(test_dataset['question'][0])\n",
    "print(test_dataset['url'][0])\n",
    "print(test_dataset['videoID'][0])\n",
    "print(test_dataset['task_type'][0])\n",
    "print(test_dataset['options'][0])\n",
    "print(test_dataset['answer'][0])\n",
    "\n",
    "import os\n",
    "\n",
    "video_data_1 = '/home/jovyan/shares/SR004.nfs2/lipengyi/Video-MME/data'\n",
    "video_data_2 = '/home/jovyan/shares/SR004.nfs2/lipengyi/Video-MME/subtitle'\n",
    "\n",
    "path = []\n",
    "for file in os.listdir(video_data_1):\n",
    "    if file.endswith(\".srt\"):\n",
    "        full_path = os.path.join(video_data_1, file)\n",
    "        path.append(full_path)\n",
    "    if file.endswith(\".mp4\"):\n",
    "        full_path = os.path.join(video_data_1, file)\n",
    "        path.append(full_path)\n",
    "\n",
    "for file in os.listdir(video_data_2):\n",
    "    if file.endswith(\".srt\"):\n",
    "        full_path = os.path.join(video_data_2, file)\n",
    "        path.append(full_path)\n",
    "    if file.endswith(\".mp4\"):\n",
    "        full_path = os.path.join(video_data_2, file)\n",
    "        path.append(full_path)\n",
    "    \n",
    "print(len(path))\n",
    "# path\n",
    "\n",
    "def find_video_path(video_id, path_list):\n",
    "    # 遍历路径列表\n",
    "    for path in path_list:\n",
    "        if video_id in path:\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "list_data = []\n",
    "for data in test_dataset:\n",
    "    my_dict = {}\n",
    "    # print(data)\n",
    "    my_dict['videoID'] = data['videoID']\n",
    "    my_dict['question'] = data['question']\n",
    "    my_dict['duration'] = data['duration']\n",
    "    my_dict['options'] = data['options']\n",
    "    my_dict['answer'] = data['answer']\n",
    "    my_dict['task_type'] = data['task_type']\n",
    "    video_path = find_video_path(data['videoID'], path)\n",
    "    my_dict['path'] = video_path\n",
    "    list_data.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6e0d9c-3af2-4d17-972c-502440706b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "\n",
    "# build_transform, dynamic_preprocess\n",
    "def build_transform(is_train, input_size, pad2square=False, normalize_type='imagenet'):\n",
    "    if normalize_type == 'imagenet':\n",
    "        MEAN, STD = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "    elif normalize_type == 'clip':\n",
    "        MEAN, STD = (0.4814546, 0.4578275, 0.40821073), (0.2686295, 0.2613025, 0.2757711)\n",
    "    elif normalize_type == 'siglip':\n",
    "        MEAN, STD = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    if is_train:  # use data augumentation\n",
    "        transform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.RandomChoice([T.Lambda(jpeg_degrade_functions[quality]) for quality in qualities]),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "    else:\n",
    "        if pad2square is False:  # now we use this transform function by default\n",
    "            transform = T.Compose([\n",
    "                T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "        else:\n",
    "            transform = T.Compose([\n",
    "                T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "                T.Lambda(lambda img: expand2square(img, tuple(int(x * 255) for x in MEAN))),\n",
    "                T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    # print(f'width: {width}, height: {height}, best_ratio: {best_ratio}')\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    \n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab5f2795-ce37-4397-a9e4-a25582cf13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoMMEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_data, prompt, question_prompt, num_segments=16, input_size=224,\n",
    "                 dynamic_image_size=False, use_thumbnail=False, max_num=6):\n",
    "        self.data_list = list_data\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.question_prompt = question_prompt\n",
    "        self.input_size = input_size\n",
    "        self.num_segments = num_segments\n",
    "        self.dynamic_image_size = dynamic_image_size\n",
    "        self.use_thumbnail = use_thumbnail\n",
    "        self.max_num = max_num\n",
    "        #self.transform = build_transform(is_train=False, input_size=input_size)\n",
    "\n",
    "        self.decord_method = {\n",
    "            'video': self.read_video,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get_index(self, bound, fps, max_frame, first_idx=0):\n",
    "        if bound:\n",
    "            start, end = bound[0], bound[1]\n",
    "        else:\n",
    "            start, end = -100000, 100000\n",
    "        start_idx = max(first_idx, round(start * fps))\n",
    "        end_idx = min(round(end * fps), max_frame)\n",
    "        seg_size = float(end_idx - start_idx) / self.num_segments\n",
    "        frame_indices = np.array([\n",
    "            int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "            for idx in range(self.num_segments)\n",
    "        ])\n",
    "        return frame_indices\n",
    "\n",
    "    \n",
    "    # 读取视频，根据对应的 video_path\n",
    "    def read_video(self, video_path, bound=None):\n",
    "        vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "        max_frame = len(vr) - 1\n",
    "        fps = float(vr.get_avg_fps())\n",
    "\n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=0)\n",
    "        for frame_index in frame_indices:\n",
    "            img = Image.fromarray(vr[frame_index].asnumpy())\n",
    "            images_group.append(img)\n",
    "\n",
    "        return images_group\n",
    "\n",
    "    # question 模版\n",
    "    def qa_template(self, data):\n",
    "        question = data['question']#f\"Question: {data['question']}\\n\"\n",
    "        question += 'Options:\\n'\n",
    "        answer = data['answer']\n",
    "        answer_idx = -1\n",
    "        for idx, c in enumerate(data['options']):\n",
    "            question += f\"{c}\\n\"\n",
    "            if c == answer:\n",
    "                answer_idx = idx\n",
    "                \n",
    "        question = question.rstrip()\n",
    "        return question, answer\n",
    "\n",
    "    # 制作数据模版\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data_item = self.data_list[idx]\n",
    "        video_path = data_item['path']\n",
    "        duration = data_item['duration']\n",
    "        # print(duration)\n",
    "        \n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Warning: Video file not found at {video_path}, skipping this item.\")\n",
    "            return None  # 或者返回一个特定的占位符数据结构\n",
    "\n",
    "    \n",
    "        #special_tokens = '\\n'.join(['Frame{}: <image>'.format(i + 1) for i in range(len(image_list))])\n",
    "        question, answer = self.qa_template(data_item)\n",
    "        #question = special_tokens + '\\n' + self.prompt + '\\n' + question + self.question_prompt\n",
    "        question = question + self.question_prompt\n",
    "    \n",
    "    \n",
    "    \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'path':video_path,\n",
    "            'task_type': data_item['task_type'],\n",
    "            'duration': duration\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64f25c7-abb1-4626-8e7c-b7f3b7094a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n'\n",
    "question_prompt = '\\nOnly give the best option.'\n",
    "\n",
    "dataset = VideoMMEDataset(\n",
    "    list_data=list_data,        \n",
    "    prompt=prompt,               \n",
    "    question_prompt=question_prompt,\n",
    "    num_segments=16,              \n",
    "    input_size=448,              \n",
    "    dynamic_image_size=True,     \n",
    "    use_thumbnail=False,          \n",
    "    max_num=6                     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8300525a-f00b-4329-a650-89ef073c23fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?Options:\\nA. Apples.\\nB. Candles.\\nC. Berries.\\nD. The three kinds are of the same number.\\nOnly give the best option.',\n",
       " 'answer': 'C',\n",
       " 'path': '/home/jovyan/shares/SR004.nfs2/lipengyi/Video-MME/data/fFjv93ACGo8.mp4',\n",
       " 'task_type': 'Counting Problem',\n",
       " 'duration': 'short'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286400d8-e347-4cc9-9acd-02bc7db449cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4846fcf-ebad-4f11-955d-631fa57a4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video LaVIT Model Weight from /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft, model precision: bf16\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bc3d780d86455d90e92dcd7980d923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.initted']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Visual Vocab Size is 16384\n",
      "The llama tokenizer vocab size is 32000\n",
      "The maximal clip number is 16\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "max_video_clips = 16\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# For Multi-Modal Understanding\n",
    "runner = build_model(model_path=model_path, model_dtype=model_dtype, understanding=True, \n",
    "        device_id=device_id, use_xformers=False, max_video_clips=max_video_clips,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e9dc7b-72ac-45e1-832f-0c31c4ceeeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Germany modern Christmas tree is initially decorated with apples, candles and berries. A total of 6 apples are used for decoration, 24 candles are used for lighting and 48 decorative berries are used. So, the option A is the best option.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "\n",
    "video_path = '/home/jovyan/shares/SR004.nfs2/lipengyi/Video-MME/data/fFjv93ACGo8.mp4'\n",
    "prompt = \"When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?\\nOptions:\\nA. Apples.\\nB. Candles.\\nC. Berries.\\nD. The three kinds are of the same number.\\nOnly give the best option.\"\n",
    "answer = \"C\"\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21840a25-5b88-46d1-8f9b-b755c83af9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples, candles, and berries are all present in equal amounts on the Germany modern Christmas tree.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?\"\n",
    "answer = \"C\"\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ccf3c7-ffd9-43ba-9ad0-722fa572480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb37797c-c525-4d94-8047-02167b3c6191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When demonstrating the Germany modern Christmas tree is initially decorated with apples, candles and berries, which kind of the decoration has the largest number?Options:\\nA. Apples.\\nB. Candles.\\nC. Berries.\\nD. The three kinds are of the same number.\\nOnly give the best option.']\n",
      "['C']\n",
      "['Counting Problem']\n",
      "['/home/jovyan/shares/SR004.nfs2/lipengyi/Video-MME/data/fFjv93ACGo8.mp4']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def collate_fn(batches, tokenizer):\n",
    "    \n",
    "    questions = [_['question'] for _ in batches]\n",
    "    video_path = [_['path'] for _ in batches]\n",
    "    answer = [_['answer'] for _ in batches]\n",
    "    task_types = [_['task_type'] for _ in batches]\n",
    "    \n",
    "    return questions, video_path, answer, task_types\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=partial(collate_fn, tokenizer=tokenizer)\n",
    "    )\n",
    "\n",
    "iterator = iter(dataloader)\n",
    "first_batch = next(iterator)\n",
    "\n",
    "\n",
    "question = first_batch[0]\n",
    "video_path = first_batch[1]\n",
    "answer = first_batch[2]\n",
    "task_type = first_batch[3]\n",
    "\n",
    "print(question)\n",
    "print(answer)\n",
    "print(task_type)\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3965c-ccb2-45d0-bff8-2771c80098b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================output====================\n",
      "A. Candles.\n",
      "====================real answers====================\n",
      "C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2700 [00:54<13:22:58, 17.86s/it]"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_real = []\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        dataloader, total=len(dataloader)\n",
    "    )\n",
    "\n",
    "for step, batch in enumerate(progress_bar, start=1):\n",
    "    question = batch[0]\n",
    "    video_path = batch[1]\n",
    "    answer = batch[2]\n",
    "    task_type = batch[3]\n",
    "\n",
    "    # print(pixel_values.size())\n",
    "    # print(question)\n",
    "    # print(answer)\n",
    "    # print(num_patches_list)\n",
    "    # print(task_type)\n",
    "    \n",
    "    # with autocast():\n",
    "        # 执行推理\n",
    "    outputs = runner({\"video\": video_path, \"text_input\": question}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "\n",
    "    y_pred.append(outputs.strip())\n",
    "    y_real.append(answer[0].strip())\n",
    "    if ((step-1)%100 == 0):\n",
    "        print(\"=\"*20 + \"output\" + \"=\"*20)\n",
    "        print(outputs)\n",
    "        print(\"=\"*20 + \"real answers\" + \"=\"*20)\n",
    "        print(answer[0], flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69326a6-f997-4403-a8e0-09acefc38ae9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_pred\u001b[49m[:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3423297e-1551-4206-b591-c82b9a7d86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InternVL-1B-16-frames-MME, accuracy is: 0.4014814814814815\n"
     ]
    }
   ],
   "source": [
    "y_preds = [i.split('.')[0] for i in y_pred]\n",
    "\n",
    "result = [1 if i == j else 0 for i, j in zip(y_real, y_preds)]\n",
    "\n",
    "print(\"VideoLAVIT-MME, accuracy is: \" + str(sum(result) / len(result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af485cc-0bd5-415f-96c4-f23bbacf1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lmms-lab/ActivityNetQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4571e48-00d8-4b15-b715-7b906357b194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_name': '1QIUV7WYKXg',\n",
       " 'question_id': 'v_1QIUV7WYKXg_3',\n",
       " 'question': 'is the athlete wearing trousers',\n",
       " 'answer': 'no',\n",
       " 'type': '3'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "553a4234-c88f-44a0-95b4-4a59ac45c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "class ActivityQaDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, question_prompt = \" The answer you give MUST be \\\"Yes\\\" or \\\"No\\\".\"):\n",
    "        \n",
    "        self.data_list = []\n",
    "        self.question_prompt = question_prompt\n",
    "\n",
    "        video_formats = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "\n",
    "        for elem in dataset:\n",
    "            for fmt in video_formats:  # Added this line\n",
    "                full_video_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/activitynet_qa/videos/all_test/v_\" + elem['video_name'] + fmt\n",
    "                if os.path.exists(full_video_path):\n",
    "                    self.data_list.append({'full_video_path':full_video_path, 'question':elem['question'], 'answer':elem['answer'], 'type':elem['type']})\n",
    "            \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data_list[idx]\n",
    "        video_path = data_item['full_video_path']\n",
    "        \n",
    "        if not os.path.exists(video_path):\n",
    "            print (video_path)\n",
    "            print(f\"Warning: Video file not found at {video_path}, skipping this item.\")\n",
    "            return None  \n",
    "    \n",
    "\n",
    "        question = data_item['question']\n",
    "        answer = data_item['answer']\n",
    "    \n",
    "        return {\n",
    "            'question': question.capitalize() + \"? \" + self.question_prompt,\n",
    "            'video_path': video_path,\n",
    "            'type':data_item['type'],\n",
    "            #'pixel_values': pixel_values,\n",
    "            'answer': answer.capitalize(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bdf09e-fbd5-4824-b771-3eb148106abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ActivityQaDataset(dataset['test'], question_prompt = \"Answer the question using a single word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c259911-8eeb-48b7-a4cd-d24fd15c903f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Is the person in white a man? Answer the question using a single word.',\n",
       " 'video_path': '/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/activitynet_qa/videos/all_test/v_mV07bEBkIcM.mp4',\n",
       " 'type': '3',\n",
       " 'answer': 'Yes'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18913888-f401-43c7-89af-b48211b4af4c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbcd376-4efe-43ad-89b4-35173395e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb4fa9d-7223-4ffe-82f0-5d6c56df34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video LaVIT Model Weight from /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft, model precision: bf16\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b38b353646e4cf9bd57abdfc8d12881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.cluster_size', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.initted']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Visual Vocab Size is 16384\n",
      "The llama tokenizer vocab size is 32000\n",
      "The maximal clip number is 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "max_video_clips = 16\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# For Multi-Modal Understanding\n",
    "runner = build_model(model_path=model_path, model_dtype=model_dtype, understanding=True, \n",
    "        device_id=device_id, use_xformers=False, max_video_clips=max_video_clips,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d3ccd3-4d5c-43c1-badc-90be5e18d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "video_path = '/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/activitynet_qa/videos/all_test/v_mV07bEBkIcM.mp4'\n",
    "prompt = 'Is the person in white a man? Answer the question using a single word.'\n",
    "\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f0c5d-265b-4f71-b5a0-35050ef072bd",
   "metadata": {},
   "source": [
    "## Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90537928-4b18-4790-8502-3505fa612490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ac9ed2-367f-41d8-9457-a941c4a31deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is the athlete wearing trousers? Answer the question using a single word.']\n",
      "['No']\n",
      "['/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/activitynet_qa/videos/all_test/v_1QIUV7WYKXg.mp4']\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def collate_fn(batches, tokenizer):\n",
    "    \n",
    "    questions = [_['question'] for _ in batches]\n",
    "    video_path = [_['video_path'] for _ in batches]\n",
    "    answer = [_['answer'] for _ in batches]\n",
    "    type_ =  [_['type'] for _ in batches]\n",
    "    \n",
    "    return questions, video_path, answer\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=partial(collate_fn, tokenizer=tokenizer)\n",
    "    )\n",
    "\n",
    "iterator = iter(dataloader)\n",
    "first_batch = next(iterator)\n",
    "\n",
    "\n",
    "question = first_batch[0]\n",
    "video_path = first_batch[1]\n",
    "answer = first_batch[2]\n",
    "\n",
    "\n",
    "print(question)\n",
    "print(answer)\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89178c19-05ff-4fb5-9845-0d7d0183affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/7990 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Is the athlete wearing trousers? Answer the question using a single word.\n",
      "====================output====================\n",
      "No\n",
      "====================real answers====================\n",
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 200/7990 [35:25<30:31:56, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "How about the dance in the video? Answer the question using a single word.\n",
      "====================output====================\n",
      "She swings her arms and legs from side to side as she dances along the water while birds land on the sidewalk beside her.\n",
      "====================real answers====================\n",
      "Good looking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 300/7990 [57:10<28:16:42, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "How many people are playing games in the video? Answer the question using a single word.\n",
      "====================output====================\n",
      "2\n",
      "====================real answers====================\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▋         | 500/7990 [1:40:25<24:37:22, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "How many people are there in the video? Answer the question using a single word.\n",
      "====================output====================\n",
      "1\n",
      "====================real answers====================\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 600/7990 [1:56:43<12:45:07,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "How many women are there in the video? Answer the question using a single word.\n",
      "====================output====================\n",
      "There is one woman in the video.\n",
      "====================real answers====================\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|█         | 800/7990 [2:35:25<25:24:57, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Is the person in the video indoors? Answer the question using a single word.\n",
      "====================output====================\n",
      "Yes\n",
      "====================real answers====================\n",
      "Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|█▏        | 900/7990 [2:53:35<16:23:17,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Does the person in blue have long hair? Answer the question using a single word.\n",
      "====================output====================\n",
      "Yes\n",
      "====================real answers====================\n",
      "No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 1000/7990 [3:13:06<20:45:00, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Is the person in the video wearing a white clothes? Answer the question using a single word.\n",
      "====================output====================\n",
      "Yes\n",
      "====================real answers====================\n",
      "Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  14%|█▍        | 1100/7990 [3:31:55<12:56:57,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================question====================\n",
      "Is the woman in the video wearing a white dress? Answer the question using a single word.\n",
      "====================output====================\n",
      "No\n",
      "====================real answers====================\n",
      "Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▍        | 1198/7990 [3:48:47<11:22:03,  6.03s/it]"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_real = []\n",
    "\n",
    "progress_bar = tqdm(\n",
    "        dataloader, total=len(dataloader), desc=f\"Epoch 1\"\n",
    "    )\n",
    "\n",
    "for step, batch in enumerate(progress_bar, start=1):\n",
    "    question = batch[0]\n",
    "    video_path = batch[1]\n",
    "    answer = batch[2]\n",
    "\n",
    "    # print(pixel_values.size())\n",
    "    # print(question)\n",
    "    # print(answer)\n",
    "    # print(num_patches_list)\n",
    "    # print(task_type)\n",
    "    \n",
    "    # with autocast():\n",
    "        # 执行推理\n",
    "    outputs = runner({\"video\": video_path[0], \"text_input\": question[0]}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "\n",
    "    y_pred.append(outputs.strip())\n",
    "    y_real.append(answer[0].strip())\n",
    "    if ((step - 1)%100 == 0):\n",
    "        with open('y_pred_actnet.pkl', 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "        with open('y_real_actnet.pkl', 'wb') as f:\n",
    "            pickle.dump(y_real, f)\n",
    "        print(\"=\"*20 + \"question\" + \"=\"*20)\n",
    "        print (question[0])\n",
    "        print(\"=\"*20 + \"output\" + \"=\"*20)\n",
    "        print(outputs)\n",
    "        print(\"=\"*20 + \"real answers\" + \"=\"*20)\n",
    "        print(answer[0], flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277ce96-3f14-410b-af22-6a5b0ff75b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926a6f8-28ef-4abb-a669-c35b80aa2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "progress_bar = tqdm(\n",
    "        dataloader, total=len(dataloader), desc=f\"Epoch 1\"\n",
    "    )\n",
    "\n",
    "for step, batch in enumerate(progress_bar, start=1):\n",
    "    question = batch[0]\n",
    "    questions.append(question[0])\n",
    "\n",
    "with open('questions_actnet.pkl', 'wb') as f:\n",
    "            pickle.dump(questions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd850f-f597-4eaf-b589-299a52f5390e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

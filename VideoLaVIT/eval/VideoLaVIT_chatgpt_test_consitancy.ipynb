{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18913888-f401-43c7-89af-b48211b4af4c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbcd376-4efe-43ad-89b4-35173395e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 2.5.1+cu124)\n",
      "    Python  3.10.11 (you have 3.10.15)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/triton/softmax.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16 if _triton_softmax_fp16_enabled else None)\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/triton/softmax.py:87: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:107: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb4fa9d-7223-4ffe-82f0-5d6c56df34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video LaVIT Model Weight from /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft, model precision: bf16\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e5610bbe38432082d3234a64ad55eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.initted', 'model.motion_tokenizer.quantize.cluster_size']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Visual Vocab Size is 16384\n",
      "The llama tokenizer vocab size is 32000\n",
      "The maximal clip number is 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from models import build_model\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "max_video_clips = 16\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# For Multi-Modal Understanding\n",
    "runner = build_model(model_path=model_path, model_dtype=model_dtype, understanding=True, \n",
    "        device_id=device_id, use_xformers=False, max_video_clips=max_video_clips,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d3ccd3-4d5c-43c1-badc-90be5e18d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/modeling_motion_tokenizer.py:433: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning, the women are standing on stage and then they start dancing after opening the curtains. From the video, we can observe that they are wearing different costumes. The video does not offer any direct source for their appearance, but we can assume that they are dressed in traditional dance costumes or dresses appropriate for belly dancing performances.\n"
     ]
    }
   ],
   "source": [
    "video_path = '/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/eval/videoChatgpt_test/v_hFi6S_guB7I.mp4'\n",
    "prompt = 'What is the appearance of the women dancers at the beginning of the video?'\n",
    "\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb9c89b-e707-4ae3-8e17-cac0f6cce11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import os\n",
    "pwd = os.path.abspath(os.getcwd())\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"consistency_qa.json\") as file:\n",
    "    gt_contents = json.load(file)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "if not os.path.exists('video_gpt_test'):\n",
    "    os.makedirs('video_gpt_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb390695-f0b3-4620-8809-08e8964ecef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 'What is the boy doing at the gym?',\n",
       " 'Q2': 'Can you describe the equipment present at the gym?',\n",
       " 'A': 'The boy is swinging between parallel bars in a well-lit gym with foam mats on the floor under various other equipment.',\n",
       " 'video_name': 'v_P7Z4i1gmzjs'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_contents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234de947-ab31-496d-93e2-da2983c07068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963b438-a5d2-48ac-b680-bf6979302bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:27, 27.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What equipment is visible in the gym where the boy is doing his routine? \n",
      " The video shows two pieces of equipment in the gym where the man is doing his routine. One is the balance beam that the man is standing on after starting the routine. The other is a trampoline that he performs a backflip over. Therefore, both pieces of equipment can be seen clearly in the video. \n",
      " There is other equipment visible in the gym like a high bar and still rings.\n",
      "Can you describe the gym where the boy is performing, and what equipment is visible? \n",
      " The gym is well-lit with light-colored walls, and various equipment is visible, including trampolines and a balance beam. \n",
      " There is other equipment visible in the gym like a high bar and still rings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [43:04, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What equipment is the man holding at the start of the video? \n",
      " The man in the video is holding a ball, which can be clearly seen in his hand. \n",
      " The man is holding a hammer with a handle attached to a string and he is practicing hammer throwing by swinging the hammer around him a few times before spinning and throwing it through the open end of the practice net.\n",
      "Can you describe what the man is doing with the hammer in the practice net? \n",
      " Sure! In the practice net, the man is turning around multiple times with the hammer to build momentum for the throw. He then spins the hammer around before throwing it out beyond the screen. \n",
      " The man is holding a hammer with a handle attached to a string and he is practicing hammer throwing by swinging the hammer around him a few times before spinning and throwing it through the open end of the practice net.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "475it [3:20:37, 15.99s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "output_list = []  # List to store the output results\n",
    "\n",
    "video_formats = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "\n",
    "output_list = []    # Iterate over each sample in the ground truth file\n",
    "for i, sample in tqdm(enumerate(gt_contents)):\n",
    "    video_name = sample['video_name']\n",
    "    sample_set = sample\n",
    "    question1 = sample['Q1']\n",
    "    question2 = sample['Q2']\n",
    "    answer = sample['A']\n",
    "\n",
    "    for fmt in video_formats:  # Added this line\n",
    "        temp_path = os.path.join(pwd + \"/videoChatgpt_test/\", f\"{video_name}{fmt}\")\n",
    "        if os.path.exists(temp_path):\n",
    "            video_path = temp_path\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        # Run inference on the video and add the output to the list\n",
    "        output1 = runner({\"video\": video_path, \"text_input\": question1}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "        output2 = runner({\"video\": video_path, \"text_input\": question2}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "    except:\n",
    "        print (\"yu\")\n",
    "\n",
    "    if (i% 100 == 0):\n",
    "        print (question1,'\\n',  output1, '\\n', answer)\n",
    "        print (question2,'\\n',  output2, '\\n', answer)\n",
    "    sample_set['pred1'] = output1\n",
    "    sample_set['pred2'] = output2\n",
    "    output_list.append(sample_set)\n",
    "\n",
    "with open(os.path.join('video_gpt_test/', \"consistency_answer.json\"), 'w') as file:\n",
    "    json.dump(output_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f0c5d-265b-4f71-b5a0-35050ef072bd",
   "metadata": {},
   "source": [
    "## Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8926a6f8-28ef-4abb-a669-c35b80aa2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Any, List, Union, Tuple, Dict, Callable\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "\n",
    "import openai\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "with open(os.path.join('video_gpt_test/', \"consistency_answer.json\"), 'r') as file:\n",
    "    output_list = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e027cdc-d3e2-4957-b3a4-16d71c729508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': 'What is the man carrying in the throwing circle?',\n",
       " 'Q2': 'Can you describe the hammer the man is using in the video?',\n",
       " 'A': 'The man is carrying a hammer attached to a handle through a metal string and he is using it as a throwing equipment.',\n",
       " 'video_name': 'v_qsEnLQ2UnEA',\n",
       " 'pred1': 'The man is carrying a large ball in a throwing circle.',\n",
       " 'pred2': 'The video does not provide information about the appearance of the hammer that the man is standing on. Therefore, I cannot answer your question based on the video.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe3c7b0-8b15-4866-be3d-4f3319dd1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71d71d3-6061-423b-9ace-b5c4c9518bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsloth/llama-3-8b-Instruct'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = await client.models.list()\n",
    "MODEL_NAME = model_names.data[0].id\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578e04b9-3e48-487a-8f40-feb5b22d550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "What equipment is visible in the gym where the boy is doing his routine? The video shows two pieces of equipment in the gym where the man is doing his routine. One is the balance beam that the man is standing on after starting the routine. The other is a trampoline that he performs a backflip over. Therefore, both pieces of equipment can be seen clearly in the video.\n",
      "None\n",
      "Can you describe the gym where the boy is performing, and what equipment is visible? The gym is well-lit with light-colored walls, and various equipment is visible, including trampolines and a balance beam.\n",
      "None\n",
      "{'score': 4}\n",
      "100\n",
      "What equipment is the man holding at the start of the video? The man in the video is holding a ball, which can be clearly seen in his hand.\n",
      "None\n",
      "Can you describe what the man is doing with the hammer in the practice net? Sure! In the practice net, the man is turning around multiple times with the hammer to build momentum for the throw. He then spins the hammer around before throwing it out beyond the screen.\n",
      "None\n",
      "{'score': 0}\n",
      "200\n",
      "What caused the laptop to fall? The laptop falls due to force exerted by the young man's body movements. He pushes the laptop at the lower part of his body, lifting it off and sending it crashing to the floor.\n",
      "None\n",
      "Why did the laptop fall? The laptop fell to the ground when the guy tried to play hopscotch while sitting on the floor. This demonstrates how technology can sometimes fall when we are not careful while using it.\n",
      "None\n",
      "{'score': 4}\n",
      "300\n",
      "What is the man in the blue jacket doing? The man in the blue jacket is raking snow off a driveway while wearing a blue jacket. He then walks away from the camera.\n",
      "None\n",
      "Can you describe the scene in the video? Certainly. In the video, a woman can be seen shoveling snow on the sidewalk of a residential area on a very snowy day. The ground is completely covered in snow. The video presents a beautiful snowy scene with cars driving down the road, trees covered with snow, and beautiful houses in the background.\n",
      "None\n",
      "{'score': 2}\n",
      "400\n",
      "What is the woman doing with the gift wrap? The woman is wrapping a box with a piece of wrapping paper and tape in the video. She places the gift box onto the wrapping paper and covers it completely with the wrapping paper. Finally, she tapes it close to finish the gift wrap.\n",
      "None\n",
      "Can you describe the process of gift wrapping performed by the woman in the video? In the video, the woman is seen wrapping a box with paper. She measures, cuts, and folds the paper along a line to reach the desired size for the gift. She then secures it with tape, ensuring that the paper is tightly wrapped around the box. She applies craft paper over the gift box, cuts it, removes the adhesive portion from the tape, and finally, she wraps the paper.\n",
      "None\n",
      "{'score': 5}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "scores = []\n",
    "for ind, elem in enumerate(output_list):\n",
    "    question1 = elem['Q1']\n",
    "    answer = elem['A']\n",
    "    pred1 = elem['pred1']\n",
    "    question2 = elem['Q2']\n",
    "    answer = elem['A']\n",
    "    pred2 = elem['pred2']\n",
    "    \n",
    "    \n",
    "    completion = await client.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\":\n",
    "                            \"You are an intelligent chatbot designed for evaluating the consistency of generative outputs for similar video-based question-answer pairs. \"\n",
    "                            \"You will be given two very similar questions, a common answer common to both the questions and predicted answers for the two questions .\"\n",
    "                            \"Your task is to compare the predicted answers for two very similar question, with a common correct answer and determine if they are consistent. Here's how you can accomplish the task:\"\n",
    "                            \"------\"\n",
    "                            \"##INSTRUCTIONS: \"\n",
    "                            \"- Focus on the consistency between the two predicted answers and the correct answer. Both predicted answers should correspond to the correct answer and to each other, and should not contain any contradictions or significant differences in the conveyed information.\\n\"\n",
    "                            \"- Both predicted answers must be consistent with each other and the correct answer, in terms of the information they provide about the video content.\\n\"\n",
    "                            \"- Consider synonyms or paraphrases as valid matches, but only if they maintain the consistency in the conveyed information.\\n\"\n",
    "                            \"- Evaluate the consistency of the two predicted answers compared to the correct answer.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\":\n",
    "                            \"Please evaluate the following video-based question-answer pair:\\n\\n\"\n",
    "                            f\"Question 1: {question1}\\n\"\n",
    "                            f\"Question 2: {question2}\\n\"\n",
    "                            f\"Correct Answer: {answer}\\n\"\n",
    "                            f\"Predicted Answer to Question 1: {pred1}\\n\"\n",
    "                            f\"Predicted Answer to Question 2: {pred2}\\n\\n\"\n",
    "                            \"Provide your evaluation only as a consistency score where the consistency score is an integer value between 0 and 5, with 5 indicating the highest level of consistency. \"\n",
    "                            \"Please generate the response in the form of a Python dictionary string with keys 'score', where its value is the consistency score in INTEGER, not STRING.\"\n",
    "                            \"DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. \"\n",
    "                            \"For example, your response should look like this: {''score': 4.8}.\"\n",
    "                    }\n",
    "                ]\n",
    "    )\n",
    "\n",
    "    # Convert response to a Python dictionary.\n",
    "    if (ind%100 ==0):\n",
    "        print (ind)\n",
    "        print (print (question1, pred1))\n",
    "        print (print (question2, pred2))\n",
    "        print (completion.choices[0].message.content)\n",
    "    try:\n",
    "        scores.append(ast.literal_eval(completion.choices[0].message.content)['score'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1b46cf-bff8-48a1-b2b6-b6ea1e603584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 5}\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f54631-39b0-4e21-9d36-ae85b468ad89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9218436873747495"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c7ba3-20ea-4783-a184-dffeccc1ce89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385ac265-3343-4038-83c8-5a3c04e75633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 2.5.1+cu124)\n",
      "    Python  3.10.11 (you have 3.10.15)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/triton/softmax.py:30: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16 if _triton_softmax_fp16_enabled else None)\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/triton/softmax.py:87: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:107: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/xformers/ops/swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from models import build_model\n",
    "from PIL import Image\n",
    "from IPython.display import Image as ipython_image\n",
    "from diffusers.utils import load_image, export_to_video, export_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c744a86a-c01c-4e3b-ba0f-b2093e5c6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversation import default_conversation, DEFAULT_IMAGE_TOKEN, DEFAULT_VIDEO_TOKEN, VIDEO_TOKEN_INDEX, IMAGE_TOKEN_INDEX\n",
    "DEFAULT_VISUAL_TOKEN = DEFAULT_VIDEO_TOKEN\n",
    "VISUAL_TOKEN_INDEX = VIDEO_TOKEN_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07853b8f-2e54-4ea7-a896-f6e3322a521d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa02b676a64424eb331538adfb4d602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/429 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecedde915494b3c81afe6098e410724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/64.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46750c5d62e44db8b847ff4cc91c190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/441337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_train = load_dataset(\"Sayankotor/video_dataset_qa\",split=\"train[:80%]\")\n",
    "dataset_valid = load_dataset(\"Sayankotor/video_dataset_qa\",split=\"train[80%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad77e375-de77-4423-9f92-07a600018d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88267"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fcb473d-423e-45f0-876b-bfc45ac00b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ink =0\n",
    "\n",
    "for elem in dataset_valid:\n",
    "    if (os.path.isfile(elem['video_path'])!= True):\n",
    "        print(elem['video_path'])\n",
    "        ink+=1\n",
    "\n",
    "ink "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d2dbf5-4389-4390-b0e9-c2e1ffc6598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling_video_lavit_hf import VideoLaVITLlamaForCausalLM\n",
    "from models.transform import LaVITImageProcessor, LaVITEvalVideoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5914a8-ed62-44ce-9652-3a4fb56e45da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c06e69e9aed4b0b88a8994f7ff1e08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.initted']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "img_size=224\n",
    "model_dtype=\"bf16\",\n",
    "apply_lemmatizer=True\n",
    "use_xformers=False\n",
    "max_frames=24\n",
    "max_clips=8\n",
    "motion_vocab_size=1026\n",
    "visual_vocab_size = 16384\n",
    "\n",
    "device_map={\"\": 0}\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "config = transformers.AutoConfig.from_pretrained(model_path)\n",
    "config.use_xformers = use_xformers\n",
    "\n",
    "# For inference, we should use the left padding\n",
    "config.tokenizer_padding_side = 'left'\n",
    "config.use_cache = True\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "model = VideoLaVITLlamaForCausalLM.from_pretrained(model_path, config=config, device_map=device_map,\n",
    "                torch_dtype=torch.bfloat16 if model_dtype==\"bf16\" else torch.float16,)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "\n",
    "visual_vocab_size = visual_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef003459-198b-4f0e-b517-b9783aa73df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transform import LaVITImageProcessor, LaVITEvalVideoProcessor\n",
    "video_processor = LaVITEvalVideoProcessor(image_size=224, num_frames=24, fps=6, max_clips=8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a737770-845b-4a0d-af8b-156131252fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b126c84-8150-452f-8683-748982d98d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset / collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77aed385-e1a9-42bb-b45c-15fda2f70dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples['text'], max_length = model.config.max_position_embeddings, truncation=True)\n",
    "    tokens['labels'] = tokens['input_ids'][len(tokens['input_ids'])//2:]\n",
    "    tokens['attention_mask'] = tokens['attention_mask'][:len(tokens['input_ids'])//2]\n",
    "    tokens['input_ids'] = tokens['input_ids'][:len(tokens['input_ids'])//2]\n",
    "    return tokens \n",
    "\n",
    "#train_dataset = ds['train'].map(#raw_datasets['validation'].map(\n",
    "#    tokenize_function,\n",
    "#    desc=\"Running tokenizer on dataset\",\n",
    "#    remove_columns=ds['train'].column_names\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd8816-2aaa-40af-8b5e-2e454fbc8049",
   "metadata": {},
   "source": [
    "## pl training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f102f9b-0723-477d-8255-6e3e47c206fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6945bbd-5a70-4d10-b78c-f6dc90be7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.video_lavit_for_understanding import tokenizer_image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a97ddb50-29eb-4590-94b0-4cc0cfaeb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538ff606-89c9-4043-bcaf-bc9910387d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2d718e0-cf8e-4ec1-ab7e-e73ad26f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "class VideoLaVIT_pl(pl.LightningModule):\n",
    "    def __init__(self, video_processor, model, train_dataset, valid_dataset, tokenizer, collate_function):\n",
    "        super(VideoLaVIT_pl, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.video_processor = video_processor\n",
    "        #self.n_embeddings = model.model.embed_tokens.weight.shape[0]\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=tokenizer.pad_token_id)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.validation_dataset = dataset_valid\n",
    "        self.collate_function = collate_function\n",
    "        self.n_iters = 100000# len(self.train_dataloader())\n",
    "        self.n_warmup = 5000\n",
    "        self.grad_clip = 1.0\n",
    "        self.weight_decay = 0.001\n",
    "        self.grad_accum = 512\n",
    "        self.lr = 0.0001\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.99\n",
    "        # self.automatic_optimization = False\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = AdamW(list(self.model.model.visual_tokenizer.parameters()) + list(self.model.model.motion_tokenizer.parameters()), lr=self.learning_rate, relative_step=False)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup, num_training_steps=self.n_iters // self.cfg.grad_accum, betas =(self.beta1, self.beta2), weight_decay = self.weight_decay)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "            \n",
    "        }}\n",
    "    \n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print (\"epoch end\")\n",
    "        #torch.save(self.model.visual_tokenizer, cwd + f\"custom_ckpts/visual_tokenizer\")\n",
    "        #torch.save(self.model.motion_tokenizer, cwd+ f\"custom_ckpts/motion_tokenizer\")\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        question = batch[0]['question']\n",
    "        answer = batch[0]['answer']\n",
    "        video_path = batch[0]['video_path']\n",
    "        visual_inputs, motion_inputs = self.video_processor(video_path)\n",
    "        imgs = [([visual_inputs, motion_inputs], 'video')]\n",
    "\n",
    "        text_inputs = [DEFAULT_VISUAL_TOKEN + \"\\n\" + text_input for text_input in [question ]]\n",
    "        \n",
    "        prompts = []\n",
    "        for text_input in text_inputs:\n",
    "            conv = default_conversation.copy()\n",
    "            conv.append_message(conv.roles[0], text_input)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        print(\"prompts\", prompts)\n",
    "\n",
    "        input_ids = [tokenizer_image_token(prompt, tokenizer, VISUAL_TOKEN_INDEX, DEFAULT_VISUAL_TOKEN, return_tensors='pt') for prompt in prompts]\n",
    "\n",
    "        print(\"input_ids:\", input_ids)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=tokenizer.pad_token_id,\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "        labels = [tokenizer(answer, return_tensors='pt').input_ids[0] for answer in answers]\n",
    "\n",
    "        print (\"answer\", answer)\n",
    "        print (\"labels\", labels)\n",
    "\n",
    "        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = self.model(input_ids = input_ids, attention_mask = attention_mask, position_ids = position_ids, past_key_values = None, inputs_embeds = None, labels = None, images = imgs).logits\n",
    "\n",
    "        print (\"logits.shape, labels.shape\", logits.shape, labels.shape)\n",
    "        #labels = labels[:, 1:]\n",
    "        #mask = mask[:, 1:]\n",
    "          \n",
    "        #logits = logits[mask].contiguous().float()\n",
    "        #labels = labels[mask].contiguous()\n",
    "\n",
    "        \n",
    "        loss = self.loss(logits, labels).mean()\n",
    "            \n",
    "        self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #print (batch[0]['question'])\n",
    "        question = batch[0]['question']\n",
    "        answers = [batch[0]['answer']]\n",
    "        video_path = batch[0]['video_path']\n",
    "        try:\n",
    "            visual_inputs, motion_inputs = self.video_processor(video_path)\n",
    "        except:\n",
    "            print (\"Exeption in loading\", video_path)\n",
    "        pass\n",
    "        imgs = [([visual_inputs, motion_inputs], 'video')]\n",
    "        \n",
    "\n",
    "        text_inputs = [DEFAULT_VISUAL_TOKEN + \"\\n\" + text_input for text_input in [question ]]\n",
    "        \n",
    "        prompts = []\n",
    "\n",
    "        for text_input in text_inputs:\n",
    "            conv = default_conversation.copy()\n",
    "            conv.append_message(conv.roles[0], text_input)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        #print(\"prompts\", prompts)\n",
    "\n",
    "        input_ids = [tokenizer_image_token(prompt, tokenizer, VISUAL_TOKEN_INDEX, DEFAULT_VISUAL_TOKEN, return_tensors='pt') for prompt in prompts]\n",
    "\n",
    "        \n",
    "\n",
    "        #print (answers)\n",
    "\n",
    "        labels = [tokenizer(answer, return_tensors='pt').input_ids[0] for answer in answers]\n",
    "\n",
    "        ids_with_labels = [torch.cat((input_id, label), 0) for input_id, label in zip(input_ids, labels)]\n",
    "\n",
    "        \n",
    "\n",
    "        ids_with_labels = torch.nn.utils.rnn.pad_sequence(ids_with_labels, batch_first=True, padding_value=tokenizer.pad_token_id).to(self.model.device)\n",
    "\n",
    "        len_labels = [len(label) for label in labels] \n",
    "\n",
    "        attention_mask = ids_with_labels.ne(tokenizer.pad_token_id)\n",
    "\n",
    "        #print (\"answer\", answers)\n",
    "        #print (\"labels\", labels)\n",
    "        #print (\"attention_mask\", attention_mask)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids = ids_with_labels, attention_mask = attention_mask, past_key_values = None, inputs_embeds = None, labels = None, images = imgs).logits\n",
    "\n",
    "        loss_mask = torch.zeros(logits.shape, device=self.model.device).bool()\n",
    "        for i, cur_len in enumerate(len_labels):\n",
    "            loss_mask[i, -cur_len:, ...] = True\n",
    "\n",
    "        labels = torch.stack(labels, dim=0).to(model.device)\n",
    "        #print (\"labels2\", labels)\n",
    "        \n",
    "\n",
    "        #print (loss_mask.shape)\n",
    "        #print (\"logits.shape\", logits.shape)\n",
    "        #print (\"mask.shape\", loss_mask.shape)\n",
    "        new_logits = logits[loss_mask].reshape((logits.shape[0], logits.shape[2],  -1))\n",
    "\n",
    "        #labels = labels[:, 1:]\n",
    "        #new_logits = new_logits[:, 1:]\n",
    "\n",
    "        #print (\"new logits.shape, labels.shape\", new_logits.shape, labels.shape)\n",
    "        #labels = labels[:, 1:]\n",
    "        #mask = mask[:, 1:]\n",
    "          \n",
    "        #logits = logits[mask].contiguous().float()\n",
    "        #labels = labels[mask].contiguous()\n",
    "\n",
    "        \n",
    "        loss = self.loss(new_logits.contiguous().float(), labels.contiguous()).mean()\n",
    "        self.log(\"val_loss\", loss, on_step=True, batch_size=1, on_epoch=True, prog_bar=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aa95b1e-c582-4c0a-8d10-d73e3657c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = VideoLaVIT_pl(video_processor, model, dataset_train, dataset_valid, tokenizer, collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f018960a-d098-497d-8745-a1776b6b5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=1, collate_fn=collate_fn, num_workers = 1)\n",
    "\n",
    "val_dataloader = DataLoader(dataset_valid, batch_size=1, collate_fn=collate_fn, num_workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87cc7c5d-8ae4-4320-a2c6-9787a846ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "logger = CSVLogger(\"ckpts\", name=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cef5517f-dda1-4626-8488-d70caaded4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(devices=1, logger =logger, log_every_n_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b7842-24e7-4521-87fe-24a40a28b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe7c7b0da4842eca6ff4b9e6ec3d9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/modeling_motion_tokenizer.py:433: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "[m4v @ 0xe7b1b40] Format m4v detected only with low score of 25, misdetection possible!\n",
      "[m4v @ 0x7f38a5a62800] Format m4v detected only with low score of 25, misdetection possible!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exeption in loading /home/jovyan/shares/SR004.nfs2/data/LLaVA-Video-178K/LLaVA-Video-178K/1_2_m_academic_v0_1/academic_source/activitynet/v_AOBkrb8yYS4.mp4\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'visual_inputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:639\u001b[0m, in \u001b[0;36mTrainer.validate\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:679\u001b[0m, in \u001b[0;36mTrainer._validate_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    676\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    678\u001b[0m )\n\u001b[0;32m--> 679\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;66;03m# remove the tensors from the validation results\u001b[39;00m\n\u001b[1;32m    681\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 109\u001b[0m, in \u001b[0;36mVideoLaVIT_pl.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExeption in loading\u001b[39m\u001b[38;5;124m\"\u001b[39m, video_path)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [([\u001b[43mvisual_inputs\u001b[49m, motion_inputs], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m    112\u001b[0m text_inputs \u001b[38;5;241m=\u001b[39m [DEFAULT_VISUAL_TOKEN \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text_input \u001b[38;5;28;01mfor\u001b[39;00m text_input \u001b[38;5;129;01min\u001b[39;00m [question ]]\n\u001b[1;32m    114\u001b[0m prompts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'visual_inputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "trainer.validate(model=module, dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37d8e631-370c-40e2-a09d-d8e88931536d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabels\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86422733-02b0-4656-85ce-c7f5ac5fc81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

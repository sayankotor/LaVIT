{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385ac265-3343-4038-83c8-5a3c04e75633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from models import build_model\n",
    "from PIL import Image\n",
    "from IPython.display import Image as ipython_image\n",
    "from diffusers.utils import load_image, export_to_video, export_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1d2aee-0695-46f6-a519-0941cd32189a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f692a7bd990>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c744a86a-c01c-4e3b-ba0f-b2093e5c6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversation import default_conversation, DEFAULT_IMAGE_TOKEN, DEFAULT_VIDEO_TOKEN, VIDEO_TOKEN_INDEX, IMAGE_TOKEN_INDEX\n",
    "DEFAULT_VISUAL_TOKEN = DEFAULT_VIDEO_TOKEN\n",
    "VISUAL_TOKEN_INDEX = VIDEO_TOKEN_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc047d7-0296-4088-afaf-ebdc017efadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset30 = load_dataset(\"lmms-lab/LLaVA-Video-178K\", '0_30_s_academic_v0_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80f5d2d-b1c4-41b1-ae0c-b7898f1f4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset30['open_ended'][4]['conversations'] # print example of conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07853b8f-2e54-4ea7-a896-f6e3322a521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "open_ended = pd.DataFrame(dataset30['open_ended'])\n",
    "caption = pd.DataFrame(dataset30['caption'])\n",
    "overall_df = pd.concat([open_ended, caption])\n",
    "dataset = Dataset.from_pandas(overall_df)\n",
    "shuffled_dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d2dbf5-4389-4390-b0e9-c2e1ffc6598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling_video_lavit_hf import VideoLaVITLlamaForCausalLM\n",
    "from models.transform import LaVITImageProcessor, LaVITEvalVideoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbd9a24-fec7-498c-9af5-0a81c134d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left') #INITIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5914a8-ed62-44ce-9652-3a4fb56e45da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ffde9548d49d68c929631eea7b99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.initted', 'model.motion_tokenizer.quantize.cluster_size']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "img_size=224\n",
    "model_dtype=\"bf16\",\n",
    "apply_lemmatizer=True\n",
    "use_xformers=False\n",
    "max_frames=24\n",
    "max_clips=8\n",
    "motion_vocab_size=1026\n",
    "visual_vocab_size = 16384\n",
    "\n",
    "device_map={\"\": 0}\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "config = transformers.AutoConfig.from_pretrained(model_path)\n",
    "config.use_xformers = use_xformers\n",
    "\n",
    "# For inference, we should use the left padding\n",
    "config.tokenizer_padding_side = 'left'\n",
    "config.use_cache = True\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "model = VideoLaVITLlamaForCausalLM.from_pretrained(model_path, config=config, device_map=device_map,\n",
    "                torch_dtype=torch.bfloat16 if model_dtype==\"bf16\" else torch.float16,)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "\n",
    "visual_vocab_size = visual_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef003459-198b-4f0e-b517-b9783aa73df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transform import LaVITImageProcessor, LaVITEvalVideoProcessor\n",
    "video_processor = LaVITEvalVideoProcessor(image_size=224, num_frames=24, fps=6, max_clips=8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b74a70-49b0-4021-9004-e00290f45f06",
   "metadata": {},
   "source": [
    "## helping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a87324-9250-4534-99b2-66e9b5b0d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversation import default_conversation, DEFAULT_IMAGE_TOKEN, DEFAULT_VIDEO_TOKEN, VIDEO_TOKEN_INDEX, IMAGE_TOKEN_INDEX, IGNORE_INDEX, MODAL_INDEX_MAP\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "import copy\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beab1b2b-0a96-43a4-a633-2d52e1331847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=VIDEO_TOKEN_INDEX, image_token=DEFAULT_IMAGE_TOKEN, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(image_token)]\n",
    "\n",
    "    #print (\"prompt\", prompt)\n",
    "    #print (\"prompt_chunks\", prompt_chunks)\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    #print (\"input_ids\", input_ids)\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddbe7ca-f919-4068-a4fa-c130ca05cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    modal_token: str = None,\n",
    ") -> Dict:\n",
    "    roles = {\"human\": \"user\", \"gpt\": \"assistant\"}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "    #print (len(sources), sources)\n",
    "    for i, source in enumerate(sources):\n",
    "        #print (\"sourse\", source, \"\\n\")\n",
    "        if roles[source[0][\"from\"]] != \"user\":\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "        message = [{'role': roles[sentence['from']], 'content': sentence['value']} for sentence in source]\n",
    "        conversation = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)\n",
    "        input_ids.append(tokenizer_image_token(conversation, tokenizer, return_tensors='pt'))\n",
    "        targets.append(copy.deepcopy(input_ids[-1]))\n",
    "\n",
    "        assert len(source) % 2 == 0, f\"Invalid conversation length {len(source)}.\"\n",
    "\n",
    "        cur = 0\n",
    "        message = []\n",
    "        for idx, sentence in enumerate(source):\n",
    "            if idx % 2 == 1:\n",
    "                tmp_message = [\n",
    "                    {'role': roles[source[idx-1]['from']], 'content': source[idx-1]['value']}, \n",
    "                    {'role': roles[sentence['from']], 'content': sentence['value']}\n",
    "                ]\n",
    "\n",
    "                instruction = tokenizer.apply_chat_template(message + tmp_message[:1], tokenize=False, add_generation_prompt=True)\n",
    "                conversation = tokenizer.apply_chat_template(message + tmp_message, tokenize=False, add_generation_prompt=False)\n",
    "                #print (\"\\n\\n\\n\",idx)\n",
    "                instruction_len = len(tokenizer_image_token(instruction, tokenizer, return_tensors='pt'))\n",
    "                conversation_len = len(tokenizer_image_token(conversation, tokenizer, return_tensors='pt'))\n",
    "\n",
    "                targets[-1][cur:instruction_len] = IGNORE_INDEX\n",
    "\n",
    "                cur = conversation_len\n",
    "                message += tmp_message\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be3ce90-2fb9-4506-a87e-64d4aff95e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(dataset30['open_ended'][3:4]['conversations'], tokenizer) <-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6e11983-83f4-4714-b2ea-88994094b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset30['open_ended'].map(lambda x: preprocess([x['conversations']], tokenizer),remove_columns=['id', 'conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f666f002-78e7-4b2a-a820-ed03acbbb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collater(instances):\n",
    "        #print (\"instances\", instances)\n",
    "        #print (\"instances0\", instances[0])\n",
    "        input_ids = instances[0][\"input_ids\"]\n",
    "        #print (\"input_ids1\", input_ids)\n",
    "        input_ids = [torch.tensor(elem) for elem in input_ids]\n",
    "        #print (\"input_ids2\", input_ids)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=tokenizer.pad_token_id)\n",
    "        labels = instances[0][\"labels\"]\n",
    "        labels = [torch.tensor(elem) for elem in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "            video_pathes = instances[0][\"video\"],\n",
    "            data_sources = instances[0][\"data_source\"]\n",
    "        )\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd8816-2aaa-40af-8b5e-2e454fbc8049",
   "metadata": {},
   "source": [
    "## pl training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a92816f-7597-469b-8655-12bf39a8ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is smoothed version of nll loss from pytorch trainer \n",
    "# TO DO: try to replace\n",
    "# https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src/transformers/trainer_pt_utils.py#L554\n",
    "def smooth_loss(logits, labels, ignore_index):\n",
    "    log_probs = -nn.functional.log_softmax(logits, dim=-1)\n",
    "    if labels.dim() == log_probs.dim() - 1:\n",
    "            labels = labels.unsqueeze(-1)\n",
    "\n",
    "    padding_mask = labels.eq(self.ignore_index)\n",
    "    # In case the ignore_index is -100, the gather will fail, so we replace labels by 0. The padding_mask\n",
    "    # will ignore them in any case.\n",
    "    labels = torch.clamp(labels, min=0)\n",
    "    nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "    # works for fp16 input tensor too, by internally upcasting it to fp32\n",
    "    smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n",
    "\n",
    "    nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "    smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "\n",
    "    # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\n",
    "    num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "    nll_loss = nll_loss.sum() / num_active_elements\n",
    "    smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "    return (1 - self.epsilon) * nll_loss + self.epsilon * smoothed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1385b0-9cd7-441d-bc43-26fdbd2b52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from models.video_lavit_for_understanding import tokenizer_image_token\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e949602c-4c2d-4eae-90fd-88090549a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"/home/jovyan/shares/SR004.nfs2/data/LLaVA-Video-178K/LLaVA-Video-178K/\"\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2d718e0-cf8e-4ec1-ab7e-e73ad26f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\"\"\"\n",
    "    Training parameters are from https://arxiv.org/html/2402.03161v3#bib.bib48 sec. A.3\n",
    "\"\"\"\n",
    "\n",
    "class VideoLaVIT_pl(pl.LightningModule):\n",
    "    def __init__(self, video_processor, model, train_dataset, tokenizer, collate_function):\n",
    "        super(VideoLaVIT_pl, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        #self.valid_dataset = valid_dataset\n",
    "        self.video_processor = video_processor\n",
    "        #self.n_embeddings = model.model.embed_tokens.weight.shape[0]\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=IGNORE_INDEX)\n",
    "        #self.train_dataset = train_dataset\n",
    "        #self.validation_dataset = dataset_valid\n",
    "        self.collate_function = collate_function\n",
    "        self.n_iters = 100000# len(self.train_dataloader())\n",
    "        self.n_warmup = 5000\n",
    "        self.grad_clip = 1.0\n",
    "        self.weight_decay = 0.001\n",
    "        self.grad_accum = 256\n",
    "        self.lr = 0.0001\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.99\n",
    "        self.collater = collate_function\n",
    "        self.n_iters = len(self.train_dataloader())\n",
    "        # self.automatic_optimization = False\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=1, collate_fn=self.collater)\n",
    "\n",
    "    def process_video(self, video_inputs, num_beams=1):\n",
    "        if not isinstance(video_inputs, list):\n",
    "            if not isinstance(video_inputs, str):\n",
    "                raise ValueError(\"The video_inputs should be Tensors or Str (video path)\")\n",
    "            video_inputs = [video_inputs]\n",
    "    \n",
    "        if isinstance(video_inputs[0], list):\n",
    "            assert isinstance(video_inputs[0][0], torch.Tensor)\n",
    "            video_inputs_list = []\n",
    "            for video_input in video_inputs:\n",
    "                for _ in range(num_beams):\n",
    "                    video_inputs_list.append(((video_input[0], video_input[1]), 'video'))\n",
    "            return video_inputs_list\n",
    "            \n",
    "        if isinstance(video_inputs[0], str):\n",
    "            video_inputs_list = []\n",
    "            for video_path in video_inputs:\n",
    "                visual_inputs, motion_inputs = video_processor(video_path)\n",
    "                for _ in range(num_beams):\n",
    "                    video_inputs_list.append(((visual_inputs, motion_inputs), 'video'))\n",
    "    \n",
    "            return video_inputs_list\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = AdamW(list(self.model.model.visual_tokenizer.parameters()) + list(self.model.model.motion_tokenizer.parameters()), lr=self.lr, betas =(self.beta1, self.beta2), weight_decay = self.weight_decay)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup, num_training_steps=self.n_iters // self.grad_accum)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "            \n",
    "        }}\n",
    "    \n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print (\"epoch end\")\n",
    "        torch.save(self.model.model.visual_tokenizer, cwd + f\"/custom_ckpts/visual_tokenizer/\" + \"model.pth\")\n",
    "        torch.save(self.model.model.motion_tokenizer, cwd+ f\"/custom_ckpts/motion_tokenizer/\" + \"model.pth\")\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        video_path = batch['video_pathes']\n",
    "        folder = batch['data_sources']\n",
    "        full_path = VIDEO_PATH + folder + \"/\" + video_path\n",
    "        visual_inputs = self.process_video(full_path)\n",
    "\n",
    "        #print (\"input_ids shape\", input_ids.shape)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = self.model(input_ids = input_ids, attention_mask = attention_mask, images = visual_inputs).logits\n",
    "\n",
    "        #print (\"logits.shape, labels.shape\", logits.shape, labels.shape)\n",
    "        \n",
    "        logits = logits[:,:labels.shape[1],:]\n",
    "\n",
    "\n",
    "        # VideoLLama2 is trained using pytorch trainer. In this Trainer, for casual LM label shifting is applied \n",
    "        logits = logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        loss = self.loss(logits.view(-1, logits.shape[2]), labels.view(-1)).mean()\n",
    "        loss.requires_grad = True\n",
    "        #print (loss)\n",
    "            \n",
    "        self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #print (batch[0]['question'])\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        video_path = batch['video_pathes']\n",
    "        folder = batch[\"data_source\"]\n",
    "        full_path = VIDEO_PATH + folder + video_path\n",
    "        print (\"FULL path\", full_path)\n",
    "        visual_inputs = self.process_video(full_path)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids = input_ids, attention_mask = attention_mask, images = visual_inputs).logits\n",
    "\n",
    "        logits = logits[:,:labels.shape[1],:]\n",
    "\n",
    "        logits = logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        loss = self.loss(logits.view(-1, logits.shape[2]), labels.view(-1)).mean()\n",
    "        loss.requires_grad = True\n",
    "\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=True, batch_size=1, on_epoch=True, prog_bar=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa95b1e-c582-4c0a-8d10-d73e3657c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = VideoLaVIT_pl(video_processor, model, a, tokenizer, collater)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96e1e0b9-236b-40f9-adad-727d8f2f6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.model.model.visual_tokenizer <-- what we are saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34068a3f-7dc2-466d-94ac-c59d13c1c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "logger = CSVLogger(\"ckpts\", name=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90af06-8b76-4ece-8793-269445828b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model | VideoLaVITLlamaForCausalLM | 8.5 B  | eval \n",
      "1 | loss  | CrossEntropyLoss           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "0         Trainable params\n",
      "8.5 B     Non-trainable params\n",
      "8.5 B     Total params\n",
      "33,847.757Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "1871      Modules in eval mode\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e61f49d63b44e61922e2b629bdcddb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(devices=1, max_epochs=1, logger =logger)\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86422733-02b0-4656-85ce-c7f5ac5fc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video LaVIT Model Weight from /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft, model precision: bf16\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372efa1411804f86bbbe63e290058c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.embedding.initted', 'model.motion_tokenizer.quantize.cluster_size']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Visual Vocab Size is 16384\n",
      "The llama tokenizer vocab size is 32000\n",
      "The maximal clip number is 16\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "max_video_clips = 16\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# For Multi-Modal Understanding\n",
    "runner = build_model(model_path=model_path, model_dtype=model_dtype, understanding=True, \n",
    "        device_id=device_id, use_xformers=False, max_video_clips=max_video_clips,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eef35666-bd0b-4361-9dc4-a4c35e79d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.model = module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0af627a7-09da-4627-aede-d07d3497204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/home/jovyan/shares/SR004.nfs2/data/LLaVA-Video-178K/LLaVA-Video-178K/0_30_s_academic_v0_1/academic_source/Charades/028CE.mp4'\n",
    "prompt = \"Where does the video take place?\"\n",
    "answer = \"In the kitchen\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54e29d-78c9-4dbf-9db5-f7471c4c56c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['USER', '<video>\\nWhere does the video take place?']]\n",
      "[['USER', '<video>\\nWhere does the video take place?'], ['ASSISTANT', None]]\n",
      "prompt A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\n",
      "Where does the video take place? ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=False, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3af028-9a58-4459-90e3-748db5fa5a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

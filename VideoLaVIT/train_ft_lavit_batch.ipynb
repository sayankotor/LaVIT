{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385ac265-3343-4038-83c8-5a3c04e75633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n",
      "Please 'pip install apex'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from models import build_model\n",
    "from PIL import Image\n",
    "from IPython.display import Image as ipython_image\n",
    "from diffusers.utils import load_image, export_to_video, export_to_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1d2aee-0695-46f6-a519-0941cd32189a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8a22885990>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c744a86a-c01c-4e3b-ba0f-b2093e5c6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversation import default_conversation, DEFAULT_IMAGE_TOKEN, DEFAULT_VIDEO_TOKEN, VIDEO_TOKEN_INDEX, IMAGE_TOKEN_INDEX\n",
    "DEFAULT_VISUAL_TOKEN = DEFAULT_VIDEO_TOKEN\n",
    "VISUAL_TOKEN_INDEX = VIDEO_TOKEN_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cc047d7-0296-4088-afaf-ebdc017efadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset30 = load_dataset(\"lmms-lab/LLaVA-Video-178K\", '0_30_s_academic_v0_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07853b8f-2e54-4ea7-a896-f6e3322a521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "open_ended = pd.DataFrame(dataset30['open_ended'])\n",
    "caption = pd.DataFrame(dataset30['caption'])\n",
    "overall_df = pd.concat([open_ended, caption])\n",
    "dataset = Dataset.from_pandas(overall_df)\n",
    "shuffled_dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d2dbf5-4389-4390-b0e9-c2e1ffc6598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling_video_lavit_hf import VideoLaVITLlamaForCausalLM\n",
    "from models.transform import LaVITImageProcessor, LaVITEvalVideoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5914a8-ed62-44ce-9652-3a4fb56e45da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> <unk>\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f7e3747dba431d9cb6e419c1924153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.embedding.initted', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.cluster_size']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "img_size=224\n",
    "model_dtype=\"bf16\",\n",
    "apply_lemmatizer=True\n",
    "use_xformers=False\n",
    "max_frames=24\n",
    "max_clips=8\n",
    "motion_vocab_size=1026\n",
    "visual_vocab_size = 16384\n",
    "\n",
    "device_map={\"\": 0}\n",
    "\n",
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "config = transformers.AutoConfig.from_pretrained(model_path)\n",
    "config.use_xformers = use_xformers\n",
    "\n",
    "# For inference, we should use the left padding\n",
    "config.tokenizer_padding_side = 'left'\n",
    "config.use_cache = True\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path, use_fast=False, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "print (tokenizer.pad_token, tokenizer.unk_token)\n",
    "\n",
    "model = VideoLaVITLlamaForCausalLM.from_pretrained(model_path, config=config, device_map=device_map,\n",
    "                torch_dtype=torch.bfloat16 if model_dtype==\"bf16\" else torch.float16,)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in model.model.visual_tokenizer.named_parameters():\n",
    "    \n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.model.motion_tokenizer.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.train()\n",
    "\n",
    "visual_vocab_size = visual_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef003459-198b-4f0e-b517-b9783aa73df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transform import LaVITImageProcessor, LaVITEvalVideoProcessor\n",
    "video_processor = LaVITEvalVideoProcessor(image_size=224, num_frames=24, fps=6, max_clips=8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b74a70-49b0-4021-9004-e00290f45f06",
   "metadata": {},
   "source": [
    "## helping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a87324-9250-4534-99b2-66e9b5b0d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversation import default_conversation, DEFAULT_IMAGE_TOKEN, DEFAULT_VIDEO_TOKEN, VIDEO_TOKEN_INDEX, IMAGE_TOKEN_INDEX, IGNORE_INDEX, MODAL_INDEX_MAP\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "import copy\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4216840c-253c-476f-942f-15999869d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_multimodal_token(prompt, tokenizer, image_token_index=VIDEO_TOKEN_INDEX, image_token=DEFAULT_IMAGE_TOKEN, return_tensors=None):\n",
    "    \"\"\"Tokenize text and multimodal tag to input_ids.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Text prompt (w/ multimodal tag), e.g., '<video>\\nDescribe the video.'\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer object.\n",
    "        multimodal_token (int): Token index corresponding to the multimodal tag.\n",
    "    \"\"\"\n",
    "    prompt_chunks = [tokenizer(chunk, add_special_tokens=False).input_ids for idx, chunk in enumerate(prompt.split(image_token))]\n",
    "\n",
    "    input_ids = []\n",
    "    for i in range(1, 2 * len(prompt_chunks)):\n",
    "        if i % 2 == 1:\n",
    "            input_ids.extend(prompt_chunks[i // 2])\n",
    "        else:\n",
    "            input_ids.append(image_token_index)\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "890858c4-2c92-41ec-b068-bc7f771c2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=VIDEO_TOKEN_INDEX, image_token=DEFAULT_IMAGE_TOKEN, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(image_token)]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ee044c-b64d-4d08-900b-1ae576e197be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(source, roles):\n",
    "    conv = default_conversation.copy()\n",
    "    for sentence in source:\n",
    "        conv.append_message(sentence['role'], sentence['content'])\n",
    "    return conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddbe7ca-f919-4068-a4fa-c130ca05cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    modal_token: str = None,\n",
    ") -> Dict:\n",
    "    roles = {\"human\": \"USER\", \"gpt\": \"ASSISTANT\"}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "    #print (len(sources), sources)\n",
    "    for i, source in enumerate(sources):\n",
    "        #print (\"sourse\", source, \"\\n\")\n",
    "        #print (\"source\", source)\n",
    "        #print (roles[source[0][\"from\"]])\n",
    "        if roles[source[0][\"from\"]] != \"USER\":\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "        #message = [{'role': roles[sentence['from']], 'content': sentence['value']} for sentence in source]\n",
    "        conversation = default_conversation.copy()#tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)\n",
    "        conv = default_conversation.copy()\n",
    "        for sentence in source:\n",
    "            conv.append_message(roles[sentence['from']], sentence['value'])\n",
    "        #print (conv.get_prompt())\n",
    "        input_ids.append(tokenizer_image_token(conv.get_prompt(), tokenizer, return_tensors='pt'))\n",
    "        targets.append(copy.deepcopy(input_ids[-1]))\n",
    "\n",
    "        #print (\"\\n input_ids\", input_ids)\n",
    "        #print (\"\\n targets\", targets)\n",
    "\n",
    "        \n",
    "\n",
    "        assert len(source) % 2 == 0, f\"Invalid conversation length {len(source)}.\"\n",
    "\n",
    "        cur = 0\n",
    "        message = []\n",
    "        for idx, sentence in enumerate(source):\n",
    "            if idx % 2 == 1:\n",
    "                tmp_message = [\n",
    "                    {'role': roles[source[idx-1]['from']], 'content': source[idx-1]['value']}, \n",
    "                    {'role': roles[sentence['from']], 'content': sentence['value']}\n",
    "                ]\n",
    "\n",
    "                #print (\"message + tmp_message[:1]\", message + tmp_message[:1])\n",
    "                #print (message + tmp_message)\n",
    "                instruction = apply_chat_template(message + tmp_message[:1], roles)#tokenizer.apply_chat_template(message + tmp_message[:1], tokenize=False, add_generation_prompt=True)\n",
    "                conversation = apply_chat_template(message + tmp_message, roles)\n",
    "\n",
    "    \n",
    "                #print (\"\\n\\n\\n\",idx)\n",
    "                #print (\"\\n\\n\",idx)\n",
    "                #print (\"instruction\", instruction)\n",
    "                #print (\"conversation\", conversation)\n",
    "                \n",
    "                instruction_len = len(tokenizer_image_token(instruction, tokenizer, return_tensors='pt'))\n",
    "                conversation_len = len(tokenizer_image_token(conversation, tokenizer, return_tensors='pt'))\n",
    "\n",
    "                targets[-1][cur:instruction_len+4] = IGNORE_INDEX\n",
    "\n",
    "                cur = conversation_len\n",
    "                message += tmp_message\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f666f002-78e7-4b2a-a820-ed03acbbb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collater(instances):\n",
    "        #print (\"instances\", instances)\n",
    "        #print (\"instances0\", instances[0])\n",
    "        input_ids = instances[0][\"input_ids\"]\n",
    "        #print (\"input_ids1\", input_ids)\n",
    "        input_ids = [torch.tensor(elem) for elem in input_ids]\n",
    "        #print (\"input_ids2\", input_ids)\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=tokenizer.pad_token_id)\n",
    "        labels = instances[0][\"labels\"]\n",
    "        labels = [torch.tensor(elem) for elem in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "            video_pathes = instances[0][\"video\"],\n",
    "            data_sources = instances[0][\"data_source\"]\n",
    "        )\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aefb2bf-34c0-4d6a-9904-426324ca8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset30['open_ended'].map(lambda x: preprocess([x['conversations']], tokenizer),remove_columns=['id', 'conversations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd8816-2aaa-40af-8b5e-2e454fbc8049",
   "metadata": {},
   "source": [
    "## pl training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a92816f-7597-469b-8655-12bf39a8ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is smoothed version of nll loss from pytorch trainer \n",
    "# TO DO: try to replace\n",
    "# https://github.com/huggingface/transformers/blob/76da6ca0349d4beb176ec6bc99d218a47980e82c/src/transformers/trainer_pt_utils.py#L554\n",
    "def smooth_loss(logits, labels, ignore_index):\n",
    "    log_probs = -nn.functional.log_softmax(logits, dim=-1)\n",
    "    if labels.dim() == log_probs.dim() - 1:\n",
    "            labels = labels.unsqueeze(-1)\n",
    "\n",
    "    padding_mask = labels.eq(self.ignore_index)\n",
    "    # In case the ignore_index is -100, the gather will fail, so we replace labels by 0. The padding_mask\n",
    "    # will ignore them in any case.\n",
    "    labels = torch.clamp(labels, min=0)\n",
    "    nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "    # works for fp16 input tensor too, by internally upcasting it to fp32\n",
    "    smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n",
    "\n",
    "    nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "    smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "\n",
    "    # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\n",
    "    num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "    nll_loss = nll_loss.sum() / num_active_elements\n",
    "    smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "    return (1 - self.epsilon) * nll_loss + self.epsilon * smoothed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae1385b0-9cd7-441d-bc43-26fdbd2b52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from models.video_lavit_for_understanding import tokenizer_image_token\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e949602c-4c2d-4eae-90fd-88090549a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"/home/jovyan/shares/SR004.nfs2/data/LLaVA-Video-178K/LLaVA-Video-178K/\"\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75bc48e7-b8e4-4eb5-9677-eac81a95a2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-201"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VISUAL_TOKEN_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19e6b2b4-d715-4dee-8d79-3b4299a5e5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 259])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(a[1]['labels'])\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b8262d3-ac6d-4ba0-878b-1a6a6bde75d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:,0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ebdad38-75fc-4ac8-9699-467c03fbffba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([], dtype=torch.int64), tensor([], dtype=torch.int64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(input_ids == VISUAL_TOKEN_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2d718e0-cf8e-4ec1-ab7e-e73ad26f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Training parameters are from https://arxiv.org/html/2402.03161v3#bib.bib48 sec. A.3\n",
    "\"\"\"\n",
    "\n",
    "class VideoLaVIT_pl(pl.LightningModule):\n",
    "    def __init__(self, video_processor, model, train_dataset, tokenizer, collate_function):\n",
    "        super(VideoLaVIT_pl, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataset = train_dataset\n",
    "        #self.valid_dataset = valid_dataset\n",
    "        self.video_processor = video_processor\n",
    "        #self.n_embeddings = model.model.embed_tokens.weight.shape[0]\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=IGNORE_INDEX)\n",
    "        #self.train_dataset = train_dataset\n",
    "        #self.validation_dataset = dataset_valid\n",
    "        self.collate_function = collate_function\n",
    "        self.n_iters = 10000# len(self.train_dataloader())\n",
    "        self.n_warmup = 1000\n",
    "        self.grad_clip = 1.0\n",
    "        self.weight_decay = 0.001\n",
    "        self.grad_accum = 256\n",
    "        self.lr = 0.0001\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.99\n",
    "        self.collater = collate_function\n",
    "        self.losses = []\n",
    "        self.n_iters = len(self.train_dataloader())\n",
    "        # self.automatic_optimization = False\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=1, collate_fn=self.collater)\n",
    "\n",
    "    def process_video(self, video_inputs, num_beams=1):\n",
    "        if not isinstance(video_inputs, list):\n",
    "            if not isinstance(video_inputs, str):\n",
    "                raise ValueError(\"The video_inputs should be Tensors or Str (video path)\")\n",
    "            video_inputs = [video_inputs]\n",
    "    \n",
    "        if isinstance(video_inputs[0], list):\n",
    "            assert isinstance(video_inputs[0][0], torch.Tensor)\n",
    "            video_inputs_list = []\n",
    "            for video_input in video_inputs:\n",
    "                for _ in range(num_beams):\n",
    "                    video_inputs_list.append(((video_input[0], video_input[1]), 'video'))\n",
    "            return video_inputs_list\n",
    "            \n",
    "        if isinstance(video_inputs[0], str):\n",
    "            video_inputs_list = []\n",
    "            for video_path in video_inputs:\n",
    "                visual_inputs, motion_inputs = video_processor(video_path)\n",
    "                for _ in range(num_beams):\n",
    "                    video_inputs_list.append(((visual_inputs, motion_inputs), 'video'))\n",
    "    \n",
    "            return video_inputs_list\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = AdamW(list(self.model.model.visual_tokenizer.parameters()) + list(self.model.model.motion_tokenizer.parameters()), lr=self.lr, betas =(self.beta1, self.beta2), weight_decay = self.weight_decay)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=self.n_warmup, num_training_steps=self.n_iters // self.grad_accum)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "            \n",
    "        }}\n",
    "    \n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print (\"epoch end\")\n",
    "        torch.save(self.model.model.visual_tokenizer.state_dict(), cwd + f\"/custom_ckpts/visual_tokenizer/\" + \"model.pth\")\n",
    "        torch.save(self.model.model.motion_tokenizer.state_dict(), cwd+ f\"/custom_ckpts/motion_tokenizer/\" + \"model.pth\")\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        video_path = batch['video_pathes']\n",
    "        folder = batch['data_sources']\n",
    "        full_path = VIDEO_PATH + folder + \"/\" + video_path\n",
    "        visual_inputs = self.process_video(full_path)\n",
    "\n",
    "        #print (\"input_ids shape\", input_ids.shape)     \n",
    "\n",
    "        #print (\"\\ninput_ids\", input_ids)     \n",
    "        \n",
    "        image_token_indices = [-1] + torch.where(input_ids == VISUAL_TOKEN_INDEX)[1].tolist() + [input_ids.shape[1]]\n",
    "        #print (\"\\nimage_token_indices!\", image_token_indices)\n",
    "        cur_labels_noim = []\n",
    "        for i in range(len(image_token_indices) - 1):\n",
    "            cur_labels_noim.append(labels[:, image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "\n",
    "\n",
    "        #print (\"\\ncur_labels_noim\", cur_labels_noim, \"\\n\")\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            logits = self.model(input_ids = input_ids, attention_mask = attention_mask, images = visual_inputs).logits\n",
    "\n",
    "\n",
    "        video_seq_len = logits.shape[1] - labels.shape[1] +1\n",
    "\n",
    "        new_labels = []\n",
    "        new_labels.append(cur_labels_noim[0])\n",
    "        new_labels.append(torch.full((1, video_seq_len), IGNORE_INDEX, device=labels.device, dtype=labels.dtype))\n",
    "        new_labels.append(cur_labels_noim[1])\n",
    "\n",
    "\n",
    "        new_labels = torch.cat(new_labels,1)\n",
    "\n",
    "        # VideoLLama2 is trained using pytorch trainer. In this Trainer, for casual LM label shifting is applied \n",
    "        #logits = logits[..., :-1, :].contiguous()\n",
    "        #new_labels = new_labels[..., 1:].contiguous()\n",
    "        \n",
    "        loss = self.loss(logits.view(-1, logits.shape[2]), new_labels.view(-1)).mean()\n",
    "        self.losses.append(loss.cpu().detach().numpy())           \n",
    "        self.log(\"my_loss\", np.mean(self.losses[-20:]), on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        #print (batch[0]['question'])\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        video_path = batch['video_pathes']\n",
    "        folder = batch[\"data_source\"]\n",
    "        full_path = VIDEO_PATH + folder + video_path\n",
    "        print (\"FULL path\", full_path)\n",
    "        visual_inputs = self.process_video(full_path)\n",
    "\n",
    "        print (\"input_ids.shape\", input_ids.shape)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids = input_ids, attention_mask = attention_mask, images = visual_inputs).logits\n",
    "\n",
    "        print (\"logits.shape\", logits.shape)\n",
    "        print (\"labels.shape\", labels.shape)\n",
    "\n",
    "        logits = logits[:,:labels.shape[1],:]\n",
    "\n",
    "        logits = logits[..., :-1, :].contiguous()\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        loss = self.loss(logits.view(-1, logits.shape[2]), new_labels.view(-1)).mean()\n",
    "        loss.requires_grad = True\n",
    "\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=True, batch_size=1, on_epoch=True, prog_bar=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aa95b1e-c582-4c0a-8d10-d73e3657c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = VideoLaVIT_pl(video_processor, model, a, tokenizer, collater)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96e1e0b9-236b-40f9-adad-727d8f2f6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.model.model.visual_tokenizer <-- what we are saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34068a3f-7dc2-466d-94ac-c59d13c1c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "logger = CSVLogger(\"ckpts\", name=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90af06-8b76-4ece-8793-269445828b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model | VideoLaVITLlamaForCausalLM | 8.5 B  | train\n",
      "1 | loss  | CrossEntropyLoss           | 0      | train\n",
      "-------------------------------------------------------------\n",
      "1.6 B     Trainable params\n",
      "6.9 B     Non-trainable params\n",
      "8.5 B     Total params\n",
      "33,847.757Total estimated model params size (MB)\n",
      "1872      Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35077367870440a0957aa8679f933ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(devices=1, max_epochs=1, max_steps=1000, log_every_n_steps=100, logger =logger)\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20c1c54c-f0f8-4239-9f49-1ad5a2002859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' After interacting with the vacuum cleaner, the person stands up, steps onto the small decorative box, stands there for a brief moment, then steps off and exits the frame. \\\\n </s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2860, 16254,   292,\n",
    "          411,   278, 11757, 29884,   398, 27372, 29892,   278,  2022, 15028,\n",
    "          701, 29892,  6576, 11480,   278,  2319, 10200,  1230,  3800, 29892,\n",
    "        15028,   727,   363,   263, 11473,  3256, 29892,   769,  6576,  1283,\n",
    "          322,   429,  1169,   278,  3515, 29889,   320, 29876, 29871,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86422733-02b0-4656-85ce-c7f5ac5fc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video LaVIT Model Weight from /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft, model precision: bf16\n",
      "Not used {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c71c9eeaaa450e817f319306e0b472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft were not used when initializing VideoLaVITLlamaForCausalLM: ['model.motion_tokenizer.quantize.embedding.initted', 'model.motion_tokenizer.quantize.embedding.cluster_size', 'model.motion_tokenizer.quantize.embedding.embed_avg', 'model.motion_tokenizer.quantize.cluster_size']\n",
      "- This IS expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoLaVITLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Visual Vocab Size is 16384\n",
      "The llama tokenizer vocab size is 32000\n",
      "The maximal clip number is 16\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/models/language_model_sft\"#\"/home/jinyang06/models/VideoLaVIT-v1/language_model_sft\"\n",
    "model_dtype='bf16'\n",
    "\n",
    "max_video_clips = 16\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# For Multi-Modal Understanding\n",
    "runner = build_model(model_path=model_path, model_dtype=model_dtype, understanding=True, \n",
    "        device_id=device_id, use_xformers=False, max_video_clips=max_video_clips,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c7132-0937-40ba-b5f2-c131d85de3f6",
   "metadata": {},
   "source": [
    "# reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a006db12-6057-4f59-a310-e0e9b3d6209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text inputs ['<video>\\nWhere does the video take place?']\n",
      "text input <video>\n",
      "Where does the video take place?\n",
      "Conversation(system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=[], offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2=' \\\\n </s>', version='v1', skip_next=False)\n",
      "[['USER', '<video>\\nWhere does the video take place?']]\n",
      "[['USER', '<video>\\nWhere does the video take place?'], ['ASSISTANT', None]]\n",
      "prompt A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\n",
      "Where does the video take place? ASSISTANT:\n",
      "prompts [\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\\nWhere does the video take place? ASSISTANT:\"]\n",
      "input_ids [tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "        29889,  3148,  1001, 29901, 29871,  -201, 29871,    13, 11921,   947,\n",
      "          278,  4863,  2125,  2058, 29973,   319,  1799,  9047, 13566, 29901])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video takes place in a garage where we see a woman wearing black and purple shoes, black clothes, and a ponytail cleaning a carpet with a vacuum cleaner. There is also another person standing on the floor next to the woman.\n"
     ]
    }
   ],
   "source": [
    "video_path = '/home/jovyan/shares/SR004.nfs2/data/LLaVA-Video-178K/LLaVA-Video-178K/0_30_s_academic_v0_1/academic_source/Charades/028CE.mp4'\n",
    "prompt = \"Where does the video take place?\"\n",
    "answer = \"In the kitchen\"\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b30e34-5b0c-47c6-bfc9-3679a8939974",
   "metadata": {},
   "source": [
    "## tuned visual tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72bb05cb-2c27-48d3-9a67-9af3e9e91d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/custom_ckpts/visual_tokenizer/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c34e89f-95ef-469a-9629-f53de7650e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(dict_['encoder.blocks.10.attn.proj.weight'], runner.model.model.visual_tokenizer.encoder.blocks[10].attn.proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab3af028-9a58-4459-90e3-748db5fa5a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.model.model.visual_tokenizer.load_state_dict(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5578a2f1-4a55-4b88-aab7-8be2709ec591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text inputs ['<video>\\nWhere does the video take place?']\n",
      "text input <video>\n",
      "Where does the video take place?\n",
      "Conversation(system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=[], offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2=' \\\\n </s>', version='v1', skip_next=False)\n",
      "[['USER', '<video>\\nWhere does the video take place?']]\n",
      "[['USER', '<video>\\nWhere does the video take place?'], ['ASSISTANT', None]]\n",
      "prompt A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\n",
      "Where does the video take place? ASSISTANT:\n",
      "prompts [\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\\nWhere does the video take place? ASSISTANT:\"]\n",
      "input_ids [tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "        29889,  3148,  1001, 29901, 29871,  -201, 29871,    13, 11921,   947,\n",
      "          278,  4863,  2125,  2058, 29973,   319,  1799,  9047, 13566, 29901])]\n",
      "The video takes place in a garage next to a white wall. The main focus of the video is on the person vacuuming, but the surrounding environment is also visible, including a white wall and a white closet.\n"
     ]
    }
   ],
   "source": [
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63568d18-c1e3-4b09-92b5-081946cb53e0",
   "metadata": {},
   "source": [
    "# tuned visual tokenizer + motion_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fe9acc4-0caa-49e3-87c9-df30f9aff0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.model.model.motion_tokenizer.load_state_dict(torch.load(\"/home/jovyan/shares/SR004.nfs2/chekalina/LaVIT/VideoLaVIT/custom_ckpts/motion_tokenizer/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2e552cc-6df3-40e7-83e8-a2ee09b248fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text inputs ['<video>\\nWhere does the video take place?']\n",
      "text input <video>\n",
      "Where does the video take place?\n",
      "Conversation(system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=[], offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2=' \\\\n </s>', version='v1', skip_next=False)\n",
      "[['USER', '<video>\\nWhere does the video take place?']]\n",
      "[['USER', '<video>\\nWhere does the video take place?'], ['ASSISTANT', None]]\n",
      "prompt A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\n",
      "Where does the video take place? ASSISTANT:\n",
      "prompts [\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <video>\\nWhere does the video take place? ASSISTANT:\"]\n",
      "input_ids [tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "        29889,  3148,  1001, 29901, 29871,  -201, 29871,    13, 11921,   947,\n",
      "          278,  4863,  2125,  2058, 29973,   319,  1799,  9047, 13566, 29901])]\n",
      "There is no information provided in the video that specifies the location. However, the woman is seen using various tools such as a vacuum, broom, and metal pan in the video.\n"
     ]
    }
   ],
   "source": [
    "video_path = '/home/jovyan/shares/SR004.nfs2/data/LLaVA-Video-178K/LLaVA-Video-178K/0_30_s_academic_v0_1/academic_source/Charades/028CE.mp4'\n",
    "prompt = \"Where does the video take place?\"\n",
    "answer = \"In the kitchen\"\n",
    "output = runner({\"video\": video_path, \"text_input\": prompt}, length_penalty=1, \\\n",
    "        use_nucleus_sampling=True, num_beams=1, max_length=512, temperature=1.0)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cf13e-e3a8-492c-9d6e-bb2aa40e20b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

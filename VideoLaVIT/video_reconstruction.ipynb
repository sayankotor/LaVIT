{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "from IPython.display import Image as ipython_image\n",
    "from models.transform import MotionVectorProcessor, extract_motions\n",
    "from diffusers.utils import export_to_video, export_to_gif\n",
    "from models import build_video_detokenizer\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local directory to save LaVIT checkpoint, set to yours\n",
    "model_path = \"/home/jinyang06/models/VideoLaVIT-v1\"\n",
    "detokenizer_weight = os.path.join(model_path, 'video_3d_unet.bin')\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)\n",
    "\n",
    "model = build_video_detokenizer(model_path, model_dtype='fp16', pretrained_weight=detokenizer_weight)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "width = 576\n",
    "height = 320\n",
    "\n",
    "max_frames = 24\n",
    "motion_transform = MotionVectorProcessor(width=width // 8, height=height // 8)\n",
    "\n",
    "pil_transform = [\n",
    "    transforms.Resize((height, width), interpolation=InterpolationMode.BICUBIC),\n",
    "]\n",
    "pil_transform = transforms.Compose(pil_transform)\n",
    "image_transform = [\n",
    "    transforms.Resize((height, width), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "]\n",
    "image_transform = transforms.Compose(image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_video_clips(video_path):\n",
    "    frames, motions, frame_types = extract_motions(video_path, raw_file=True, temp_dir='./tmp', fps=12)\n",
    "    total_frames = len(frame_types)\n",
    "    start_indexs = np.where(np.array(frame_types)=='I')[0]\n",
    "    \n",
    "    if len(start_indexs) == 0:\n",
    "        raise ValueError(f\"Empty Start indexs: {video_path}\")\n",
    "\n",
    "    # FIlter one I-Frame + 11 P-Frame\n",
    "    if len(start_indexs) > 1:\n",
    "        end_indexs = start_indexs + 12\n",
    "        filter_start_indexs = start_indexs[:-1][end_indexs[:-1] == start_indexs[1:]]    \n",
    "    else:\n",
    "        filter_start_indexs = start_indexs\n",
    "\n",
    "    # FIlter the frames that exceed the max frames\n",
    "    filter_start_indexs = filter_start_indexs[filter_start_indexs + max_frames <= total_frames]\n",
    "\n",
    "    if len(filter_start_indexs) > 0:\n",
    "        # Stack the motions\n",
    "        start_index = np.random.choice(filter_start_indexs)\n",
    "        indices = np.arange(start_index, start_index + max_frames)\n",
    "        motions = [torch.from_numpy(motions[i].transpose((2,0,1))) for i in indices]\n",
    "        motions = torch.stack(motions).float()\n",
    "        motions = motion_transform(motions)\n",
    "        filtered_frames = [Image.fromarray(frames[i]).convert(\"RGB\") for i in indices]\n",
    "        pil_frames = [pil_transform(frame) for frame in filtered_frames]\n",
    "        frame_tensors = [image_transform(frame) for frame in filtered_frames]\n",
    "        frame_tensors = torch.stack(frame_tensors)\n",
    "        frame_tensors = 2.0 * frame_tensors - 1.0\n",
    "        return pil_frames, frame_tensors, motions\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Empty Filtered Start indexs: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'demo/31200691.mp4'\n",
    "\n",
    "video_frames, video_tensors, motions = sample_video_clips(video_path)\n",
    "output_video_path = \"original.gif\"\n",
    "export_to_gif(video_frames, output_video_path)\n",
    "display(ipython_image(open(output_video_path,'rb').read()))\n",
    "keyframe = video_tensors[0:1]\n",
    "motions = motions.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames =  model.reconstruct_from_token(keyframe.to(\"cuda\"), motions.to(\"cuda\"), decode_chunk_size=8, \n",
    "        width=width, height=height, num_frames=24, noise_aug_strength=0.02, cond_on_ref_frame=True, \n",
    "        use_linear_guidance=True, max_guidance_scale=3.0, min_guidance_scale=1.0,)[0]\n",
    "output_video_path = \"reconstruct.gif\"\n",
    "export_to_gif(frames, output_video_path)\n",
    "display(ipython_image(open(output_video_path,'rb').read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
